{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenida\n",
    "\n",
    "Bienvenido al segundo modulo del programa profesional en inteligencia artificial. En este modulo exploraremos algoritmos de aprendizaje profundo.\n",
    "En esta lección vamos a aprender sobre las redes neuronales, de sus estructura y requerimientos para funcionar. \n",
    "Vamos a iniciar con la red neuronal mas pequeña: La red neuronal para una tabla de verdad (“Truth Table”) que todos Uds. conocen de sus cursos de lógica. \n",
    "\n",
    "## Inspiración: \n",
    "\n",
    "Las redes neuronales son llamadas así, debido a la interacción de objetos (grafos) que simulan conexiones cerebrales (según Alan Turing en 1948). En realidad, los procesos cerebrales son mucho más complejos, así que el concepto de “red neuronal” utilizado comúnmente en Deep Learning, no representa realmente la naturaleza del funcionamiento del cerebro, pero la abstracción y su implementación permiten realizar labores de clasificación de excelente calidad. \n",
    "\n",
    "La siguiente foto es una representación de una neurona en el cerebro. \n",
    "\n",
    "<img src=\"img/neurona.png\"  width=\"600\" />\n",
    "\n",
    "De la imagen se extraen las siguientes ideas:\n",
    "- El núcleo recibe señales (datos) de otras neuronas\n",
    "- El núcleo capaz de ejecutar un proceso y emite una nueva señal (amplificada o reducida)\n",
    "- El núcleo puede enviar la señal a otras neuronas\n",
    "\n",
    "De esto, podemos construir un modelo de neurona que podemos abstraer de la siguiente forma: \n",
    "\n",
    "<img src=\"img/neurona2.png\" />\n",
    "\n",
    "Vamos a modificar nuestra representación para incluir el concepto de los coeficientes y el “step function”. Normalmente los coeficientes en algoritmos de machine learning son llamados betas o thetas, pero en las redes neuronales, les llamamos pesos o weights. Debido al termino anglosajón, llamamos a los pesos *W*. \n",
    "\n",
    "<img src=\"img/neurona3.png\" width=\"600\"/>\n",
    "\n",
    "## Mi Primera Red Neuronal con Python\n",
    "\n",
    "**Tabla de Verdad OR**\n",
    "\n",
    "<img src=\"img/tabla.png\" width=\"300\"/>\n",
    "\n",
    "Vamos a implementar una red neuronal para predecir la tabla de verdad *OR*. Vamos a simular para este ejemplo, que tenemos los datos de entrada [T,F] osea, {1,O}. Recordemos que la funcion Sigmoid implementa $Z = X^T * W$ donde X y W deben tener el mismo tamano. Como los datos de entrada son solo 2 {1,0}, debemos agregar un 1 al inicio para que el peso del intercept no se afecte. En la siguiente imagen, se muestra nuestra neurona:\n",
    "\n",
    "<img src=\"img/neurona4.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "El modelo implica que tenemos dos vectores, X y W cada uno del mismo tamaño. Para este ejercicio inicial usaremos unos pesos “pre-calculados” para facilitar la intuición. Mas adelante, en otro notebook aprenderemos sobre el algoritmo de “Backpropagation” para estimar los persos w. Por ahora asumiremos los pesos W = [10, 20, 20] y los datos de entrada X = [1, 1, 0]\n",
    "\n",
    "A continuación, vamos a declarar nuestros vectores y la función sigmoid que ud trabajo en el primer curso de AI01 - Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# datos\n",
    "X = np.array([1,1,0])\n",
    "\n",
    "# pesos\n",
    "W = np.array([10,20,20])\n",
    "\n",
    "# function Sigmoid\n",
    "def Sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# implementacion de la preduccion \n",
    "def Prediccion(x,w):\n",
    "    y = 0\n",
    "    prob = Sigmoid(np.dot(x,w))\n",
    "    if (prob >= 0.5):\n",
    "        y = 1\n",
    "    return [prob,y]\n",
    "    \n",
    "# Ejecucion de predicion {1,0}\n",
    "\n",
    "print(\"El valor [1 OR 0] es:\", Prediccion(X,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección hemos aprendido que una red neuronal (de una sola neurona) comparte conceptos muy similares a Logisitic Regression. En los siguientes notebooks exploraremos que las redes neuronales con mas de una neurona pueden resolver problemas complejos de clasificación. \n",
    "También hemos observado que los pesos W, que no fueron calculados en esta ocasión, provienen de un algoritmo llamado “Backpropagation” que tiene similitudes con el algoritmo de “Gradient Descent” que aprendimos en el primer curso. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
