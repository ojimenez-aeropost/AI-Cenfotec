{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Activación\n",
    "\n",
    "Las funciones de activación son ecuaciones matemáticas que determinan la salida de una red neuronal. La función se adjunta a cada neurona de la red y determina si debe activarse (\"dispararse\") o no, en función de si la entrada de cada neurona es relevante para la predicción del modelo. Las funciones de activación también ayudan a normalizar la salida de cada neurona en un rango entre 1 y 0 o entre -1 y 1.\n",
    "\n",
    "Un aspecto adicional de las funciones de activación es que deben ser eficientes desde el punto de vista computacional porque se calculan en miles o incluso millones de neuronas para cada muestra de datos. Las redes neuronales modernas utilizan una técnica llamada retropropagación para entrenar el modelo, lo que aumenta la tensión computacional sobre la función de activación y su función derivada.\n",
    "\n",
    "A continuación, vamos a conocer las funciones de activación mas populares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Es una función matemática que permite modelar los datos en un rango de [0,1]. Se utiliza como una función de probabilidad donde se utiliza comúnmente el valor de 0.5 para determinar sin un valor x pertenece a una clase.  En el caso de las redes neuronales, su valor decimal se conserva y no se asocia con un uno o cero como en el caso de logistic regression.\n",
    "Algunas de las características de la función es que es diferenciable, lo que indica que su pendiente puede ser calculada, es monotónica (solo crece o decrece), los valores de salida no están centrados en cero, y se ha reportado que en algunas arquitecturas neuronales, puede ralentizar el aprendizaje.\n",
    " \n",
    "\n",
    "$$\\mathrm{sig} \\left(x \\right) = \\frac{1}{1 + e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "plt.plot(X,sig(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh (Tangente Hiperbólica)\n",
    "\n",
    "Tanh es la función de tangente hiperbólica, que es el análogo hiperbólico de la función circular Tan utilizada en trigonometría. Tanh se define como la relación de las funciones de seno hiperbólico y coseno hiperbólico correspondientes a través de $tanh(x) = \\frac{sinh(x)}{cosh(x)}$. Tanh también se puede definir como $tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}$, donde *e* es la base del logaritmo natural Log.\n",
    "\n",
    "Tanh produce una salida similar a Sigmoid pero en el rango de [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.sinh(x)/np.cosh(x)\n",
    "\n",
    "plt.plot(X,tanh(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU es la función de activación mas utilizada en redes neuronales, redes neuronales profundas y convolucionales. Su función es muy sencilla: $R(z)=max(0,z)$, es correcto, si el valor es cero o negativo esta función devuelve cero, de lo contrario, simplemente devuelve el valor original. El rango de respuesta es de $[0, \\infty]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "plt.plot(X,relu(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "La función ReLU tiene un problema, todos los valores negativos son siempre cero. Esto en ciertas ocasiones afecta la calidad de las predicciones ya que el efecto de los valores negativos puede tener un impacto. \n",
    "Este problema con ReLU se llama “The Dying ReLU problem” y afecta la capacidad del algoritmo de backpropagation cuando los datos de entrada se acercan a cero o son negativos. \n",
    "\n",
    "Para resolver el problema de ReLU, la formula de Leaky ReLU ha modificado la formula de la siguiente forma: $R(z)=max(0.1z,z)$\n",
    "\n",
    "Esto permite tener visibles y controlados los valores negativos. Es decir, si van a tener una representación que no es cero, pero cercano a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky(x):\n",
    "    return np.maximum(0.1*x, x)\n",
    "\n",
    "plt.plot(X,leaky(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
