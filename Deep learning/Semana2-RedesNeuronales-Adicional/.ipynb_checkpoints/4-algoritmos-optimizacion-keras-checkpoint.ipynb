{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de Optimización \n",
    "\n",
    "Durante nuestro primer curso IA01 Machine Learning, exploramos un algoritmo de optimización con dos variaciones: descenso de gradiente y descenso de gradiente estocástico. \n",
    "\n",
    "Los optimizadores son algoritmos o métodos que se utilizan para cambiar los atributos de una red neuronal, como los pesos ycon el fin de reducir las pérdidas (loss).\n",
    "\n",
    "Keras no solo soporta descenso de gradiente, sino que también cuenta con la siguiente lista de algoritmos de optimización. \n",
    "\n",
    "- **Stocastic Gradient Descent (SGD)**\n",
    "- **RMSprop**\n",
    "- **Adam**\n",
    "- **Adamax**\n",
    "- Adadelta\n",
    "- Adagrad\n",
    "- Nadam\n",
    "- Ftrl \n",
    "\n",
    "Donde podemos parametrizar el algoritmo de 2 formas:\n",
    "\n",
    "**1:**\n",
    "\n",
    "- model.compile(loss='sparse_categorical_crossentropy', optimizer='**adam**', metrics=['accuracy'])\n",
    "\n",
    "**2:** \n",
    "\n",
    "- opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "- model.compile(loss='sparse_categorical_crossentropy', optimizer=**opt**, metrics=['accuracy'])\n",
    "\n",
    "Es importante observar que la segunda forma nos permite configurar una lista de hyperparametros para cada algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametros\n",
    "\n",
    "<img src=\"img/general.png\" width=\"400\" />\n",
    "\n",
    "#### SGD\n",
    "\n",
    "Optimizador de descenso de gradientes (con impulso).\n",
    "\n",
    "- learning_rate\n",
    "\n",
    "V:\n",
    "\n",
    "- Fácil implementación \n",
    "- Fácil de entender\n",
    "\n",
    "D:\n",
    "\n",
    "- Susceptible al local mínima\n",
    "- Se debe calcular el gradiente de todos los datos * (se reduce en SGD)\n",
    "- Requiere grandes cantidades de memoria * (se reduce en SGD)\n",
    "\n",
    "\n",
    "#### RMSprop \n",
    "\n",
    "<img src=\"img/rms.png\" width=\"400\" />\n",
    "\n",
    "La esencia de RMSprop es:\n",
    "\n",
    "Mantener un promedio móvil (descontado) del cuadrado de gradientes\n",
    "Divida el gradiente por la raíz de este promedio\n",
    "\n",
    "- learning_rate (Default 0.001.)\n",
    "- rho (Default  0.9) (Decay Rate)\n",
    "- epsilon (Default 1e-7)\n",
    "\n",
    "#### Adam (Adaptive Momentum Estimation)\n",
    "\n",
    "<img src=\"img/adam.png\" width=\"500\" />\n",
    "\n",
    "La optimización de Adam es un método de descenso de gradiente estocástico que se basa en la estimación adaptativa de momentos de primer y segundo orden.\n",
    "\n",
    "- learning_rate (Default 0.001.)\n",
    "- beta_1 (Default  0.9) exponential decay rate\n",
    "- beta_2 (Default  0.999) exponential decay rate\n",
    "- epsilon (Default 1e-7) constante para evitar division por cero\n",
    "\n",
    "V:\n",
    "\n",
    "- Converge rápido\n",
    "\n",
    "D:\n",
    "\n",
    "- Computacionalmente costoso\n",
    "\n",
    "\n",
    "#### Adamax\n",
    "\n",
    "Es una variante de Adam basada en la norma del infinito. Los parámetros predeterminados siguen los proporcionados en el paper (https://arxiv.org/abs/1412.6980). Adamax es a veces superior a Adam, especialmente en modelos con incrustaciones.\n",
    "\n",
    "- learning_rate (Default 0.001.)\n",
    "- rho (Default  0.9)\n",
    "- beta_1 (Default  0.9)\n",
    "- beta_2 (Default  0.999)\n",
    "- epsilon (Default 1e-7)\n",
    "\n",
    "V:\n",
    "\n",
    "- Puede funcionar mejor que Adam si los datos son muy ruidosos\n",
    "\n",
    "D:\n",
    "\n",
    "- Computacionalmente costoso\n",
    "\n",
    "### Benchmark de Convergencia\n",
    "\n",
    "<img src=\"img/opt1.gif\" />\n",
    "\n",
    "<img src=\"img/fnlevels.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
