{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Confusión\n",
    "\n",
    "Una matriz de confusión es una tabla que se puede utilizar para medir el rendimiento de un algoritmo de aprendizaje automático, generalmente uno de aprendizaje supervisado (de clasificación). Cada fila de la matriz de confusión representa las instancias de una clase real y cada columna representa las instancias de una clase predicha.\n",
    "\n",
    "<img src=\"img/mat.jpg\"/>\n",
    "\n",
    "Actualmente hemos estado realizando la métrica de exactitud como el total de los valores correctamente predichos sobre el total de todos los valores de la muestra. A pesar de que es una métrica válida, es incompleta y no brinda mayor detalle sobre lo que realmente está haciendo el algoritmo. \n",
    "\n",
    "La matriz de confusión nos proporciona más información (métricas) que nos permite identificar el tipo de errores que realiza nuestro modelo y para identificar si existe algún problema con algún tipo de clase (desequilibrios).\n",
    "\n",
    "Veamos las métricas que podemos obtener de una matriz de confusión: \n",
    "\n",
    "Para ejemplificar, supongamos tenemos una **funcion f(foto) -> [cancer, no-cancer]**, que permite identificar si una foto de una mancha en la piel corresponde o no a un caso de cáncer.\n",
    "\n",
    "### Valores de la Matriz:\n",
    "\n",
    "- **True Positive (TP)**: El algoritmo recibe una foto como parámetro y lo clasifica como un cáncer. Cada predicción de este tipo suma 1 a TP.\n",
    "- **<font color='red'>False Negative (FN)</font>**: El algoritmo recibe una foto como parámetro y lo clasifica como no-cáncer. Esto se denomina como error del tipo II, ya que la foto si ha sido clasificada como un melanoma.\n",
    "- **True Negative (TN)**: El algoritmo recibe una foto como parámetro y lo clasifica como un no-cáncer. Efectivamente, la foto no corresponde a un caso maligno.\n",
    "- **<font color='red'>False Positive (FP)</font>**: El algoritmo recibe una foto como parámetro e indica que es cáncer, pero en realidad no lo es. Esto se denomina como error del tipo I. Normalmente los errores de tipo I son considerados errores de menor severidad que los errores de tipo II (sin embargo ambos son errores). \n",
    "\n",
    "### Metricas Adicionales (probabilidades marginales)\n",
    "\n",
    "- **Precision:** Responde a la pregunta: ¿qué proporción de las identificaciones positivas (predichas), son realmente correctas? \n",
    "- **Recall (*Sensitivity*):** Responde a la pregunta \"¿Qué proporción de positivos reales se identificó correctamente?\"\n",
    "- **Specificity:**: Similar al Recall, busca idenficar la tasa de los negativos actuales que fueron clasificados correctamente.  \n",
    "- **Negative Predicted Value:** indica la probabilidad de que una foto que fue clasificada como \"no-cancer\" realmente no sea maligna. \n",
    "- **<font color='green'>Accuracy:</font>** El la probabilidad de que de que el modelo clasifique correctamente (para las clases existentes)\n",
    "\n",
    "\n",
    "### Otras Metricas:\n",
    "\n",
    "$$F1Score = \\frac{2*precision*recall}{precision+recall} = \\frac{2*TP}{2*TP+FP+FN}$$\n",
    "\n",
    "- **F1-Score**: Es una medida de la precisión de una prueba: es la media armónica de *precision* y *recall*. Puede tener una puntuación máxima de 1 (*precision* y *recall* perfectos) y un mínimo de 0. En general, es una medida de la precisión y robustez de su modelo. \n",
    "\n",
    "Importante:\n",
    "\n",
    "- *Accuracy* se usa cuando los verdaderos positivos (TP) y los verdaderos negativos (TN) son más importantes, mientras que la *F1Score* se usa cuando los falsos negativos y los falsos positivos son cruciales.\n",
    "- *Accuracy* se puede utilizar cuando la distribución de clases es similar, mientras que el *F1Score* es una mejor métrica cuando hay clases desequilibradas (imbalanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso de Matriz de Confusion: SVM con Kernel Linear vs Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "#classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)\n",
    "classifier = LogisticRegression(multi_class='multinomial', max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                             display_labels=class_names,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None)\n",
    "plt.show()\n",
    "\n",
    "print(\"Desplegar Matriz\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prima = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_prima, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_prima)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
