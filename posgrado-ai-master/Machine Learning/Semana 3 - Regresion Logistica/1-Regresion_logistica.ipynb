{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística\n",
    "\n",
    "Hasta ahora, hemos construído modelos supervisados de **regresión**, es decir, modelos en los que la salida de la función que aprendemos es numérica. Esta semana haremos nuestros primeros modelos de **clasificación**, en los que la salida es categórica, utilizando regresión logística.\n",
    "\n",
    "Véamos un ejemplo. Tenemos un dataset con dos columnas, `horas_estudio` y `resultado_examen`. `horas_estudio` es una variable numérica que representa el número de horas que un estudiante le ha dedicado a prepararse para un examen, mientras que `resultado_examen` es una variable binaria que puede tomar uno de los varlores `APROBADO` o `REPROBADO`. \n",
    "\n",
    "| `horas_estudio` | `resultado_examen` |\n",
    "| --------------- | ------------------ |\n",
    "| 0 | `REPROBADO` |\n",
    "| 7 | `APROBADO` |\n",
    "| 3 | `REPROBADO` |\n",
    "| 4 | `APROBADO` |\n",
    "| ... | ... |\n",
    "| 2 | `REPROBADO` |\n",
    "\n",
    "Nuestro objetivo es **crear un modelo para determinar si un estudiante aprobará o no el examen**, utilizando como entrada el número de horas que ha dedicado a prepararse. Específicamente, aprenderemos una función $f(\\texttt{horas}\\_\\texttt{estudio}) \\mapsto \\left[0, 1\\right]$ que devuelve la probabilidad de que el resultado del examen sea `APROBADO`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutemos esta celda para cargar el dataset y las bibliotecas básicas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "horas_estudio = [5, 4, 1, 1, 1, 2, 3, 3, 2, 0, 3, 3, 2, 3, 2, 3, 1, 5, 4, 4, 3, 4, 2, 4, 3, 3, 6, 4, 4, 5, 3, 4, 3, 2, 6, 2, 4, 4, 1, 5, 3, 3, 5, 7, 4, 3, 5, 2, 3, 5, 3, 4, 3, 3, 1, 3, 6, 0, 4, 3, 7, 2, 5, 3, 3, 4, 3, 1, 5, 4, 1, 4, 3, 1, 2, 4, 2, 2, 2, 2, 4, 3, 3, 1, 4, 2, 1, 0, 4, 4, 3, 3, 4, 6, 5, 3, 2, 2, 6, 6]\n",
    "resultado_examen = ['APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas importantes\n",
    "\n",
    "- El nombre _regresión_ logística suele confundir. Estrictamente *sí* es un modelo de regresión que modela una variable numérica (la probabilidad), pero lo utilizamos para construir un clasificador\n",
    "- La regresión logística es un **modelo probabilístico** (la función que aprendemos retorna una probabilidad). Otros modelos, p.ej. los árboles de decisión, producen directamente las etiquetas\n",
    "- La regresión logística modela una salida binaria, p.ej. `APROBADO`/`REPROBADO`, `TRUE`/`FALSE`. Para más de dos categorías, el modelo general es la **regresión Softmax** que estudiaremos más adelante\n",
    "- Un detalle importante en un modelo probabilístico es el \"threshold\" o **valor que marca la frontera entre una etiqueta de clasificación y la otra**. Esto lo estudiaremos con más detalle en el notebook sobre curvas ROC. Por ahora, fijaremos este valor en 0.5 ya que es el número en el que la probabilidad de una clase domina a la otra.  \n",
    "\n",
    "## Ingredientes para regresión logística\n",
    "\n",
    "Usaremos como base la función lineal de regresión $f(x) = \\beta_0 + \\beta_1 x$, con dos ingredientes adicionales:\n",
    "\n",
    "1. Una función de enlace o activación que nos garantiza que la salida estará entre 0 y 1\n",
    "2. Una función de error para minimizar, adecuada al tipo de datos binario de la salida.\n",
    "\n",
    "### La función logística\n",
    "\n",
    "Existe una familia entera de funciones llamadas sigmoides. Se les llama de esta manera porque al graficarlas producen una **línea en forma de \"S\"**. La función logística es una de ellas. De hecho, en mucha de la literatura de Machine Learning se le conoce simplemente como \"función sigmoide\". Se define de la siguiente manera: $$\\mathrm{sig} \\left(x \\right) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Su derivada con es $$\\mathrm{sig}' \\left(x \\right) = \\mathrm{sig} \\left(x \\right) \\left(1 - \\mathrm{sig} \\left( x \\right) \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos la función logística (sigmoide) y su derivada\n",
    "def sig(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def d_sig(x):\n",
    "    return sig(x) * (1.0 - sig(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la función d_sig(0) es 0.25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grafiquemos la función logística. \n",
    "# Notemos cómo el valor del eje 'y' está entre 0 y 1. Notemos también que sig(0)=0.5\n",
    "x_val = np.linspace(-10, 10)\n",
    "plt.plot(x_val, sig(x_val))\n",
    "\n",
    "print(\"la función d_sig(0) es\", d_sig(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de entropía cruzada\n",
    "\n",
    "El segundo ingrediente para hacer regresión logística es tener una función de error que tome el lugar de MSE y sea **más adecuada para una respuesta binaria**. Para esto, denotemos la variable $y$ como un valor original obtenido del dataset y supongamos que dichos valores han sido codificados adecuadamente (siguiendo nuestro ejemplo, `1=\"APROBADO\"` y `0=\"REPROBADO\"`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codifiquemos la variable 'resultado_examen' de modo que APROBADO=1.0 y REPROBADO=0.0\n",
    "resultado_examen_enc = np.array([1.0 if y == 'APROBADO' else 0.0 for y in resultado_examen])\n",
    "resultado_examen_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar, denotemos la variable $\\hat{y}$ como una predicción realizada por nuestro modelo. A diferencia de $y$, que puede tomar solo los valores 0 o 1, $\\hat{y}$ puede tomar cualquier valor en el intervalo $[0,1]$. Queremos **cuantificar el error de predecir $\\hat{y}$ cuando el valor real es $y$**, así $$\\mathrm{bce}\\left(y, \\hat{y} \\right)=\\begin{cases}-\\textrm{log}(\\hat{y}) &\\textrm{si } y=1 \\\\ -\\textrm{log}(1 - \\hat{y}) &\\textrm{si } y=0\\end{cases}$$\n",
    "\n",
    "**Nota**: aquí $\\textrm{log}$ se refiere a la función de logaritmo natural. Para referirnos a la función logística, usaremos el término $\\textrm{sig}$.\n",
    "\n",
    "Tratemos de entender lo que sucede aquí. Cuando el valor real $y=1$, el error de nuestra predicción $\\hat{y}$ tenderá a cero a medida que $\\hat{y} \\rightarrow 1$, al mismo tiempo que tenderá a infinito a medida que $\\hat{y} \\rightarrow 0$. Lo mismo sucede vice-versa cuando $y=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Cuando y_real = 1.0 | y_pred = 0.99 \t--> error = 0.01005033585350145\n",
      "Cuando y_real = 1.0 | y_pred = 0.8 \t--> error = 0.2231435513142097\n",
      "Cuando y_real = 1.0 | y_pred = 0.5 \t--> error = 0.6931471805599453\n",
      "Cuando y_real = 1.0 | y_pred = 0.2 \t--> error = 1.6094379124341003\n",
      "Cuando y_real = 1.0 | y_pred = 0.01 \t--> error = 4.605170185988091\n",
      " \n",
      "Cuando y_real = 0.0 | y_pred = 0.99 \t--> error = 4.605170185988091\n",
      "Cuando y_real = 0.0 | y_pred = 0.8 \t--> error = 1.6094379124341005\n",
      "Cuando y_real = 0.0 | y_pred = 0.5 \t--> error = 0.6931471805599453\n",
      "Cuando y_real = 0.0 | y_pred = 0.2 \t--> error = 0.2231435513142097\n",
      "Cuando y_real = 0.0 | y_pred = 0.01 \t--> error = 0.01005033585350145\n"
     ]
    }
   ],
   "source": [
    "# Probemos la función de error de entropía cruzada con distintos valores\n",
    "def bce_simple(y_real, y_pred):\n",
    "    if math.isclose(y_real, 1.0):\n",
    "        return -math.log(y_pred)\n",
    "    \n",
    "    return -math.log(1.0 - y_pred)\n",
    "\n",
    "for rv in [1.0, 0.0]:\n",
    "    print(' ')\n",
    "    for pv in [0.99, 0.8, 0.5, 0.2, 0.01]:\n",
    "        print('Cuando y_real =', rv, '| y_pred =', pv, '\\t--> error =', bce_simple(rv, pv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función de error se puede representar también de la forma $$\\mathrm{bce}(y, \\hat{y}) = -y \\mathrm{log}(\\hat{y}) - (1-y) \\mathrm{log}(1 - \\hat{y})$$\n",
    "siendo su derivada con respecto a $\\hat{y}$ $$\\frac{\\partial \\mathrm{bce}(y, \\hat{y})}{\\partial \\mathrm{\\hat{y}}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}.$$\n",
    "\n",
    "Para efectos de un dataset compuesto por $N$ observaciones, la función de error sería $$\\mathrm{bce}(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{N} -y_i \\mathrm{log}(\\hat{y_i}) - (1-y_i) \\mathrm{log}(1 - \\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos la función de error de entropía cruzada para dataset enteros representados como arreglos de numpy\n",
    "def bce(y_real, y_pred):\n",
    "    return np.sum(-y_real*np.log(y_pred) - (1.0 - y_real)*np.log(1.0 - y_pred))\n",
    "\n",
    "def d_bce(y_real, y_pred):\n",
    "    return (y_pred - y_real) / (y_pred * (1.0-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mezclando los ingredientes\n",
    "\n",
    "Ya hemos definido los dos ingredientes adicionales para hacer regresión logística. Volvamos al planteamiento inicial sobre la ecuación de regresión $f(x) = \\beta_0 + \\beta_1 x$. En regresión logística, extendemos esta ecuación con la función logística, teniendo así el modelo $$g(x) = \\mathrm{sig} \\left( \\beta_0 + \\beta_1 x \\right).$$\n",
    "Nuestro objetivo de optimización es **encontrar los valores de $\\beta_0$ y $\\beta_1$** que minimizan el valor de la función objetivo $\\mathbf{bce}$ sobre un dataset dado.\n",
    "\n",
    "## Estimación de parámetros \n",
    "\n",
    "Volvamos a nuestro ejemplo del exámen y las horas de estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos nuestro diccionario global de parámetros.\n",
    "dic_param = {\n",
    "    'beta1': 0.0,\n",
    "    'beta0': 0.0,\n",
    "}\n",
    "\n",
    "# Definamos nuestro diccionario global de constantes\n",
    "dic_const = {\n",
    "    'threshold': 0.5, # frontera límite de clasificación--si valor es mayor, clasifica como APROBADO.\n",
    "    'lr': 0.01, # tasa de aprendizaje para gradiente descendiente (cc cpmo Alpha/Alfa)\n",
    "    'num_iter': 500 # número de iteraciones\n",
    "}\n",
    "\n",
    "# Retorna la probabilidad de APROBADO\n",
    "def prob_aprobado(horas_estudio_x):    \n",
    "    return sig(horas_estudio_x * dic_param['beta1'] + dic_param['beta0'])\n",
    "\n",
    "# Retorna la etiqueta APROBADO/REPROBADO\n",
    "def label_aprobado(horas_estudio):\n",
    "    return 'APROBADO' if prob_aprobado(horas_estudio) > dic_const['threshold'] else 'REPROBADO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los valores de $\\beta_0$ y $\\beta_1$, utilizaremos el algoritmo de gradiente descendiente. La manera estándar de hacerlo es encontrando las derivadas de la función de costo contra $\\alpha$ y $\\beta$ utilizando la regla de la cadena de cálculo: \n",
    "$$\\frac{\\partial \\mathrm{bce}(y, \\hat{y})}{\\partial \\beta_1} = \\frac{\\partial \\mathrm{bce}(y, \\hat{y})}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial f} \\times \\frac{\\partial f}{\\partial \\beta_1}$$\n",
    "y del mismo modo con $\\beta_0$. Lo interesante es que **nosotros no tenemos que encontrar esta derivada simbólicamente**, sino simplemente multiplicar los valores numéricos resultantes de evaluar por separado las derivadas de las funciones de regresión, error y activación, teniendo estas últimas dos ya implementadas como ``d_sig`` y ``d_bce``. Como veremos más adelante, este \"truco\" es la base del algoritmo *backpropagation* que se utiliza para encontrar los parámetros de las redes neuronales. Aunque nos falta implementar las derivadas de $f(x)$ para $\\beta_0$ y $\\beta_1$, estas son $x$ y $1$, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los gradientes de b0 y b1, los devuelve como una tupla de la forma (b1,b0)\n",
    "# Recibe el dataset (x, y_real) y los valores actuales de b1 y b0\n",
    "def grads(x, y_real, cur_b1, cur_b0):\n",
    "    \n",
    "    # Calculamos el valor la función de regresión\n",
    "    f_val = x * cur_b1 + cur_b0\n",
    "    \n",
    "    # Generamos nuestra predicción\n",
    "    y_pred = sig(f_val)\n",
    "    \n",
    "    # Obtenemos el gradiente del error de dicha predicción\n",
    "    d_err = d_bce(y_real, y_pred)\n",
    "    \n",
    "    # Obtenemos el gradiente de la logística \n",
    "    d_f_val = d_sig(f_val)\n",
    "    \n",
    "    # Y los gradientes con respecto a b1 y b0 usando la regla de la cadena\n",
    "    # Los sumamos para obtener un gradiente escalar que podemos utilizar en nuestra actualización de parámetros\n",
    "    d_b1 = np.sum(d_err * d_f_val * x)\n",
    "    d_b0 = np.sum(d_err * d_f_val)\n",
    "    \n",
    "    return (d_b1, d_b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ya tenemos todo listo para armar nuestro algoritmo de gradiente descendiente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x, y):\n",
    "    \n",
    "    # Inicializamos alpha y beta utilizando una distribución normal estándar\n",
    "    [b1, b0] = np.random.randn(2)\n",
    "    lr = dic_const['lr']\n",
    "    \n",
    "    # Lista de error por iteración (para graficar)\n",
    "    errs = []\n",
    "    \n",
    "    for i in range(dic_const['num_iter']):\n",
    "        \n",
    "        # Calculamos las predicciones y el error para todo el dataset\n",
    "        pred = sig(b1*x + b0)\n",
    "        err = bce(y, pred)\n",
    "        \n",
    "        # Calculamos los gradientes\n",
    "        (d_b1, d_b0) = grads(x, y, b1, b0)\n",
    "        \n",
    "        # Actualizamos los parámetros usando los gradientes y la tasa de aprendizaje\n",
    "        b1 = b1 - lr * d_b1\n",
    "        b0 = b0 - lr * d_b0\n",
    "        \n",
    "        # Guardamos el error de esta iteración\n",
    "        errs.append(err)\n",
    "        \n",
    "    \n",
    "    # Actualizamos los valores de los parámetros para salida\n",
    "    dic_param['beta1'] = b1\n",
    "    dic_param['beta0'] = b0\n",
    "    \n",
    "    return errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejecutamos nuestra optimización con descenso de gradientes y graficamos la curva de error entre iteraciones\n",
    "errs = gd(np.array(horas_estudio), resultado_examen_enc)\n",
    "plt.plot(errs)\n",
    "plt.show()\n",
    "\n",
    "# Probamos nuestro modelo contra distintas horas de estudio y graficamos la probabilidad\n",
    "hora_test = np.linspace(0, 10)\n",
    "hora_prob = prob_aprobado(hora_test)\n",
    "plt.plot(hora_test, hora_prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si ud estudia durante 3.5 horas, ud tiene una probabilidad de pasar de 0.8326475285642749 lo que indica que estara APROBADO\n",
      "Coeficientes b0: -4.466837030812945 b1: 1.7346700722407113\n",
      "BCE: 37.65195677278731\n"
     ]
    }
   ],
   "source": [
    "# Realizamos una prediccion:\n",
    "horas = 3.5\n",
    "hora_prob = prob_aprobado(horas)\n",
    "pred = label_aprobado(horas)\n",
    "\n",
    "print(\"Si ud estudia durante\",horas, \n",
    "      \"horas, ud tiene una probabilidad de pasar de\",hora_prob,\"lo que indica que estara\",pred)\n",
    "\n",
    "print(\"Coeficientes b0:\",dic_param['beta0'], \"b1:\",dic_param['beta1'])\n",
    "print(\"BCE:\", errs[len(errs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo puede predecir el dataset completo con una exactitud del  92.0 %\n"
     ]
    }
   ],
   "source": [
    "# Revisar la prediccion sobre el dataset. (warning: hacer esto en train/test set en las practicas)\n",
    "\n",
    "# variable dependiente (prediccion)\n",
    "y_prima = [label_aprobado(p) for p in horas_estudio]\n",
    "\n",
    "# variable dependiente\n",
    "y = resultado_examen\n",
    "\n",
    "# exactitud\n",
    "acc = np.sum((np.array(y_prima) == np.array(y)) * 1) / len(y)\n",
    "\n",
    "print(\"El algoritmo puede predecir el dataset completo con una exactitud del \", acc * 100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística multivariable\n",
    "\n",
    "Ya que tenemos nuestro primer modelo, vamos a hacer una extensión importante: multiples variables de entrada. Supongamos, que además de ``horas_estudio``, tenemos otras variables adicionales en nuestro dataset: ``horas_sueno`` y ``num_repeticiones`` que representan respectivamente el número de horas de sueño antes de tomar el examen, y el número de veces que el estudiante ha tomado el examen anteriormente (``0`` si es la primera vez que lo toma). De modo que ahora la función que debemos aprender es $f\\left(\\texttt{horas}\\_\\texttt{estudio}, \\texttt{horas}\\_\\texttt{sueno}, \\texttt{num}\\_\\texttt{repeticiones} \\right) \\mapsto \\left[0, 1 \\right]$.\n",
    "\n",
    "Esto cambia el problema de varias maneras:\n",
    "\n",
    "1. $\\mathbf{x}$ en nuestro dataset pasa de ser un vector de $N$ valores, a una matriz $X$ de $N \\times 3$\n",
    "2. $\\beta_i$ pasa de ser un valor escalar y se convierte en un vector $\\boldsymbol{\\beta}$ de $3$ valores.\n",
    "3. Nuestro modelo es ahora $g(\\mathbf{x}) = \\mathrm{sig} \\left(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_0 \\right) = \\mathrm{sig}\\left(\\boldsymbol{\\beta_i}^{\\mathrm{T}}\\mathbf{x} + \\beta_0 \\right)$\n",
    "\n",
    "Veamos primero como quedan nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "horas_sueno = [4, 4, 5, 5, 4, 6, 7, 7, 5, 8, 5, 7, 7, 7, 7, 7, 4, 8, 4, 8, 6, 8, 5, 4, 5, 5, 7, 4, 6, 8, 6, 4, 7, 4, 5, 7, 7, 4, 6, 6, 5, 6, 7, 6, 4, 8, 7, 4, 4, 6, 5, 8, 6, 8, 4, 5, 4, 6, 8, 5, 6, 6, 5, 6, 4, 8, 4, 6, 5, 7, 5, 6, 7, 7, 8, 4, 4, 6, 8, 6, 4, 6, 5, 5, 5, 4, 4, 7, 8, 5, 4, 8, 5, 6, 5, 6, 7, 7, 4, 5]\n",
    "repeticiones = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Combinamos los tres arreglos 'horas_estudio', 'horas_sueno' y 'repeticiones' en una matriz de Nx3\n",
    "datos_mul = np.array((horas_estudio, horas_sueno, repeticiones)).transpose()\n",
    "\n",
    "# Definimos un diccionario de parámetros nuevo para el caso multivariable\n",
    "dic_param_mul = {\n",
    "    'bi': np.array([0.0, 0.0, 0.0]), # ahora es un vector\n",
    "    'b0': 0.0 # permanece igual\n",
    "}\n",
    "\n",
    "# Definamos nuestro diccionario global de constantes\n",
    "dic_const_mul = {\n",
    "    'threshold': 0.5, # frontera límite de clasificación--si valor es mayor, clasifica como APROBADO.\n",
    "    'lr': 0.001, # tasa de aprendizaje para gradiente descendiente -- MAS PEQUEÑA EN ESTE CASO\n",
    "    'num_iter': 1000 # número de iteraciones -- MÁS ALTA EN ESTE CASO\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cambio más importante viene a la hora de aplicar la función de regresión. Para poder multiplicar el vector $\\boldsymbol{\\beta_i}$ contra el vector de valores $\\mathbf{x}$ y producir un valor escalar, utilizaremos la función ``numpy.dot``. Este cambio lo debemos hacer en nuestra función de evaluación de gradientes y en la de optimización gradiente descendiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los gradientes de alpha y beta, los devuelve como una tupla de la forma (alpha,beta)\n",
    "# Recibe el dataset (x, y_real) y los valores actuales de alpha y beta\n",
    "def grads_mul(x, y_real, cur_bi, cur_b0):\n",
    "    \n",
    "    # Calculamos el valor la función de regresión\n",
    "    # OBSERVEMOS ESTE CAMBIO contra la implementación anterior\n",
    "    ### ANTES: f_val = x * cur_alpha + cur_beta\n",
    "    ### AHORA:\n",
    "    f_val = np.dot(x, cur_bi) + cur_b0\n",
    "    \n",
    "    # Generamos nuestra predicción\n",
    "    y_pred = sig(f_val)\n",
    "    \n",
    "    # Obtenemos el gradiente del error de dicha predicción\n",
    "    d_err = d_bce(y_real, y_pred)\n",
    "    \n",
    "    # Obtenemos el gradiente de la logística \n",
    "    d_f_val = d_sig(f_val)\n",
    "    \n",
    "    # Y los gradientes con respecto a alpha y beta usando la regla de la cadena\n",
    "    # Los sumamos para obtener un gradiente escalar que podemos utilizar en nuestra actualización de parámetros\n",
    "    ### ANTES: d_b1 = np.sum(d_err * d_f_val * x)\n",
    "    ### AHORA:\n",
    "    d_bi = np.dot(d_err * d_f_val, x)\n",
    "    d_b0 = np.sum(d_err * d_f_val)\n",
    "    \n",
    "    return (d_bi, d_b0)\n",
    "\n",
    "# Ahora cambiemos la función de obtención de gradientes\n",
    "def gd_mul(x, y):\n",
    "    \n",
    "    # Inicializamos alpha y beta utilizando una distribución normal estándar\n",
    "    # CAMBIO: los inicializamos ahora por separado\n",
    "    bi = np.random.randn(3)\n",
    "    [b0] = np.random.randn(1)\n",
    "    lr = dic_const_mul['lr']\n",
    "    \n",
    "    # Lista de error por iteración (para graficar)\n",
    "    errs = []\n",
    "    \n",
    "    for i in range(dic_const_mul['num_iter']):\n",
    "        \n",
    "        # Calculamos las predicciones y el error para todo el dataset\n",
    "        # CAMBIO: función de regresión usando np.dot\n",
    "        ### ANTES: pred = sig(alpha*x + beta)\n",
    "        pred = sig(np.dot(x, bi) + b0)\n",
    "        err = bce(y, pred)\n",
    "        \n",
    "        # Calculamos los gradientes\n",
    "        ### CAMBIO: utilizamos 'grads_mul' en vez de 'grads'\n",
    "        (d_bi, d_b0) = grads_mul(x, y, bi, b0)\n",
    "        \n",
    "        # Actualizamos los parámetros usando los gradientes y la tasa de aprendizaje\n",
    "        bi = bi - lr * d_bi\n",
    "        b0 = b0 - lr * d_b0\n",
    "        \n",
    "        # Guardamos el error de esta iteración\n",
    "        errs.append(err)\n",
    "\n",
    "    # Actualizamos los valores de los parámetros para salida\n",
    "    dic_param_mul['betai'] = bi\n",
    "    dic_param_mul['beta0'] = b0\n",
    "    \n",
    "    return errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119d30310>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAATIklEQVR4nO3dbYxtVX3H8e//zHAvFS0PMqXIpb0YiZaaWsmEQmwbIlaRGuEFMRATb5WGNDHV2jYK8QVpUpOSGhGTlkpEpY0BldpCSKtStPWV2KEaRR7kKkUu4WGQh1ZM4d45/744a8+cfWaPc++cGeauzfeTTM7Zaz/99+zJb69ZZ8+eyEwkSf0y2O4CJEmbz3CXpB4y3CWphwx3Seohw12Semh2uwsAOP7443P37t3bXYYkVeXOO+98IjPnuuYdFuG+e/duFhYWtrsMSapKRDy41jyHZSSphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknqo6nD/wWP/y8e+eh9P/PS57S5Fkg4rVYf7/Y/9lE98bS9PPvv8dpciSYeVqsO94f8bkaS2qsM9YrsrkKTDU9Xh3kjsukvSuKrD3Y67JHWrOtwbjrlLUlvV4e6YuyR1qzrcG/bcJamt8nC36y5JXSoP9xHvlpGktqrD3TF3SepWdbg3HHOXpLaqw92OuyR1qzrcJUndqg73cNBdkjpVHe4Nx9wlqa3qcLffLkndqg73hve5S1Jb1eHukLskdas63BuOuUtS27rhHhGfjojHI+Kusba/joh7I+K7EfFPEXHM2LzLI2JvRNwXEW/ZorrLvrZy65JUr4PpuX8WOHei7TbgtZn5G8APgMsBIuI04CLg18s6fxsRM5tW7RrsuEtS27rhnpnfAJ6caPtqZh4ok98EdpX35wM3ZuZzmfkAsBc4YxPrbQnvl5GkTpsx5v4e4F/L+5OAh8bm7SttWyoddJeklqnCPSI+DBwAPreBdS+NiIWIWFhcXNxgARtbTZL6bsPhHhF/ALwNeGeudJ0fBk4eW2xXaVslM6/NzPnMnJ+bm9toGaNtTbW2JPXPhsI9Is4FPgi8PTN/NjbrFuCiiNgZEacApwLfmr7MNerYqg1LUuVm11sgIm4AzgaOj4h9wBWM7o7ZCdxWHt71zcz8o8z8fkR8Abib0XDNezNzaauKlyR1WzfcM/Pijubrfs7yHwE+Mk1Rh8rPUyWpreq/UPWRv5LUrepwX2HXXZLGVR3u9tslqVvV4d5wzF2S2qoOd4fcJalb1eHesOMuSW1Vh7sPDpOkblWHe8Mxd0lqqzrcHXOXpG5Vh3vDR/5KUlvV4W7HXZK6VR3uDfvtktRWd7jbdZekTnWHe+GQuyS1VR3u3ucuSd2qDvdGOuouSS1Vh7v3uUtSt6rDfZkdd0lqqTrc7bhLUreqw71hx12S2qoOd/+HqiR1qzrcG97nLkltVYe7HXdJ6lZ1uDe8z12S2qoOdzvuktSt6nBvOOYuSW1Vh7tj7pLUrepwb9hxl6S2ysPdrrskdak83Ef8H6qS1FZ1uDvmLkndqg73hv12SWqrOtztuEtSt3XDPSI+HRGPR8RdY23HRcRtEXF/eT22tEdEfCIi9kbEdyPi9K0sfpldd0lqOZie+2eBcyfaLgNuz8xTgdvLNMBbgVPL16XANZtTZjefCilJ3dYN98z8BvDkRPP5wPXl/fXABWPtf58j3wSOiYgTN6nWtWu06y5JLRsdcz8hMx8p7x8FTijvTwIeGltuX2lbJSIujYiFiFhYXFzcUBH22yWp29QfqOboJvND7jpn5rWZOZ+Z83Nzc1PWMNXqktQ7Gw33x5rhlvL6eGl/GDh5bLldpW1LOOQuSd02Gu63AHvK+z3AzWPt7yp3zZwJPDM2fLNl7LlLUtvsegtExA3A2cDxEbEPuAL4K+ALEXEJ8CDwjrL4vwDnAXuBnwHv3oKaV2pz1F2SOq0b7pl58RqzzulYNoH3TlvUobLjLkltdf+Fqh13SepUdbg3fCqkJLX1ItwlSW29CHf77ZLUVnW4O+YuSd2qDveGQ+6S1FZ1uHufuyR1qzrcV9h1l6RxVYe7Y+6S1K3qcJckdetFuPuBqiS1VR3uDstIUreqw71hx12S2qoOd2+FlKRuVYd7wzF3SWqrOtwdc5ekblWHeyMddZeklqrD3Y67JHWrOtwbjrlLUlvV4e6YuyR1qzrcG3bcJamt8nC36y5JXSoP9xH/QbYktVUd7o65S1K3qsNdktSt6nC34y5J3aoO94ZD7pLUVnW4h4PuktSp6nBv+GwZSWqrOtztt0tSt6rDveGYuyS1VR3uDrlLUreqw71hz12S2qYK94j4QER8PyLuiogbIuLIiDglIu6IiL0R8fmI2LFZxa7av6PuktRpw+EeEScB7wPmM/O1wAxwEXAlcFVmvgp4CrhkMwr9eey4S1LbtMMys8AvRMQs8BLgEeCNwE1l/vXABVPuY02OuUtStw2He2Y+DHwU+DGjUH8GuBN4OjMPlMX2ASd1rR8Rl0bEQkQsLC4ubrSMppap1pekvplmWOZY4HzgFOAVwFHAuQe7fmZem5nzmTk/Nze30TIkSR2mGZZ5E/BAZi5m5n7gS8AbgGPKMA3ALuDhKWtcl/12SWqbJtx/DJwZES+J0UNezgHuBr4OXFiW2QPcPF2Ja3PMXZK6TTPmfgejD07/C/he2da1wIeAP42IvcDLges2oc51itnyPUhSVWbXX2RtmXkFcMVE84+AM6bZ7sHyqZCS1K0ff6Fq112SWqoOd/vtktSt6nBveJu7JLVVHe4OuUtSt6rDvWHHXZLaqg53nwopSd2qDveGY+6S1FZ1uDvmLkndqg73hve5S1Jb1eFux12SulUd7g3H3CWpre5wt+suSZ3qDvfCjrsktVUd7t7nLkndqg73ZQ66S1JL1eHufe6S1K3qcG/Yb5ektqrD3Y67JHWrOtwbDrlLUlvV4e7/UJWkblWHeyPtuktSS9Xhbr9dkrpVHe6SpG69CHcHZSSprepw9/NUSepWdbg3/DxVktqqDncfHCZJ3aoO94Ydd0lqqzvc7bhLUqe6w73wj5gkqa3qcPduGUnqVnW4S5K6VR3udtwlqdtU4R4Rx0TETRFxb0TcExFnRcRxEXFbRNxfXo/drGLX4pC7JLVN23O/GvhyZr4GeB1wD3AZcHtmngrcXqa3hI/8laRuGw73iDga+F3gOoDMfD4znwbOB64vi10PXDBdietL73SXpJZpeu6nAIvAZyLi2xHxqYg4CjghMx8pyzwKnNC1ckRcGhELEbGwuLi4oQLst0tSt2nCfRY4HbgmM18PPMvEEEyObkDv7FZn5rWZOZ+Z83Nzc1OU4Zi7JE2aJtz3Afsy844yfROjsH8sIk4EKK+PT1fi2hxyl6RuGw73zHwUeCgiXl2azgHuBm4B9pS2PcDNU1V4MLVs9Q4kqTKzU67/x8DnImIH8CPg3YwuGF+IiEuAB4F3TLmPNflUSEnqNlW4Z+Z3gPmOWedMs91Dr+OF3JskHf7q/gtVO+6S1KnqcG94n7sktfUi3CVJbVWHezMs45i7JLVVHe6Dku7+sw5JautFuC8Nt7kQSTrMVB7uo9ehPXdJaqk63COCCIdlJGlS1eEOo6GZJcNdklqqD/eZCIZmuyS1VB/uETA03SWppfpwnxmEH6hK0oTqw33gsIwkrVJ9uEfAkukuSS3Vh/vMILwVUpImVB/u3gopSav1INxxzF2SJvQg3B2WkaRJvQh3P1CVpLbqw310n/t2VyFJh5fqwz3Cp0JK0qTqw30Q4eMHJGlC9eHusIwkrVZ9uEfgfe6SNKH6cJ/xVkhJWqX6cB+NuW93FZJ0eKk+3B2WkaTVqg93HxwmSatVH+7+haokrdaDcPfBYZI0qf5w99/sSdIq9Yd7GO6SNKn6cJ/xVkhJWmXqcI+ImYj4dkTcWqZPiYg7ImJvRHw+InZMX+bP27+3QkrSpM3oub8fuGds+krgqsx8FfAUcMkm7GNN/rMOSVptqnCPiF3A7wOfKtMBvBG4qSxyPXDBNPtYjw8Ok6TVpu25fxz4INCMer8ceDozD5TpfcBJXStGxKURsRARC4uLixsu4MgjBvzs+aUNry9JfbThcI+ItwGPZ+adG1k/M6/NzPnMnJ+bm9toGRx31A6efPa5Da8vSX00O8W6bwDeHhHnAUcCvwhcDRwTEbOl974LeHj6Mtd23FE7efLZ58lMRqNCkqQNh3tmXg5cDhARZwN/npnvjIgvAhcCNwJ7gJunL3Ntx790B/uXkr/7jx/xSy/byY7ZwfLXzpnR6xEzK207ZgbsnG1PzwzCC4OkXpmm576WDwE3RsRfAt8GrtuCfSx706+dwDX//kOu/PK9U21ndhDMDGLldWbAbDM9E8wOBmPzgpnBgCMOcrq5eMwMRvflDwbBTKzdPhgEg9I+iJVtNO8HsX57ex8svx9vH0Qw6KhpMLatKK+jL4hYmdeev7K8F0pp+8XhcBvh/Px8LiwsbHj9/UtDfvLT5/m//Us8vzTk+QPDldcDK9P7l4Y8N9H2/IEhB4bJ0rC8LiUHhsmB4ZClYXJgeboss5QsDZP960w32ziwlAwzWRpSXpPhMFnKUftwOLpPv08PPxsP/PUvBuPzy/KDQ1y+a/uDQ1y+a/uD9ZcnIGi2vfKeZn8066xc9Fa2szI/Ioix711nW3nftK8sV2phpd7xmmDl2ANGxzWx3/FtrdRc2sp2J+tY2We7IzBZ53hNy/voapuss/n+Tm63rMPE9HKdY/vvu4i4MzPnu+ZtRc/9BXfEzIBfPvrI7S5jak3oLw2TzJXQX74YDJNhaR8OR/Na7cNcvoCs175U5i1fcJr5Y/vOMj9zZdlhUqZX2jJZ3tdBLz/WluUid0jLd21/7AK6f+kglu/a/vDgll8aJsmojWT5fWaSwGHQZ1LRGfw0F+a1LxCMT3dsg9Y6q7exvO91tn/xGb/CH/7OKzf9uHsR7n0xGAQDgiNmtrsSbYbMduA3FwZoX1Sai0GWtvFlxy8WK8s289pto1/+mu2WC85w9Nosv/oiNLbP4UqdTNYx1j7ZlmW/w46amn0u19TVRvtYmzqabbHq+FfWYeIYxudlWbGrvZlmebp7G8vbX2MbLNd+ENtfbl+ZJmHuZTs39wevMNylLRIRzDRdNOkFVv2DwyRJqxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPXRYPFsmIhaBBze4+vHAE5tYTg085hcHj/nFYZpj/tXM7PyHGIdFuE8jIhbWenBOX3nMLw4e84vDVh2zwzKS1EOGuyT1UB/C/drtLmAbeMwvDh7zi8OWHHP1Y+6SpNX60HOXJE0w3CWph6oO94g4NyLui4i9EXHZdtezWSLi5Ij4ekTcHRHfj4j3l/bjIuK2iLi/vB5b2iMiPlG+D9+NiNO39wg2JiJmIuLbEXFrmT4lIu4ox/X5iNhR2neW6b1l/u5tLXwKEXFMRNwUEfdGxD0RcVafz3NEfKD8TN8VETdExJF9PM8R8emIeDwi7hprO+TzGhF7yvL3R8SeQ6mh2nCPiBngb4C3AqcBF0fEadtb1aY5APxZZp4GnAm8txzbZcDtmXkqcHuZhtH34NTydSlwzQtf8qZ4P3DP2PSVwFWZ+SrgKeCS0n4J8FRpv6osV6urgS9n5muA1zE6/l6e54g4CXgfMJ+ZrwVmgIvo53n+LHDuRNshndeIOA64Avgt4AzgiuaCcFCy/EPj2r6As4CvjE1fDly+3XVt0bHeDPwecB9wYmk7EbivvP8kcPHY8svL1fIF7Co/8G8EbmX0v+meAGYnzzfwFeCs8n62LBfbfQwbOOajgQcma+/reQZOAh4Cjivn7VbgLX09z8Bu4K6NnlfgYuCTY+2t5db7qrbnzsoPSmNfaeuV8qvo64E7gBMy85Ey61HghPK+D9+LjwMfBIZl+uXA05l5oEyPH9Py8Zb5z5Tla3MKsAh8pgxHfSoijqKn5zkzHwY+CvwYeITRebuT/p/nxqGe16nOd83h3nsR8VLgH4E/ycz/GZ+Xo0t5L+5jjYi3AY9n5p3bXcsLbBY4HbgmM18PPMvKr+pA787zscD5jC5qrwCOYvXQxYvCC3Feaw73h4GTx6Z3lbZeiIgjGAX75zLzS6X5sYg4scw/EXi8tNf+vXgD8PaI+G/gRkZDM1cDx0TEbFlm/JiWj7fMPxr4yQtZ8CbZB+zLzDvK9E2Mwr6v5/lNwAOZuZiZ+4EvMTr3fT/PjUM9r1Od75rD/T+BU8sn7TsYfTBzyzbXtCkiIoDrgHsy82Njs24Bmk/M9zAai2/a31U+dT8TeGbs17/DXmZenpm7MnM3o/P4tcx8J/B14MKy2OTxNt+HC8vy1fVuM/NR4KGIeHVpOge4m56eZ0bDMWdGxEvKz3hzvL0+z2MO9bx+BXhzRBxbfut5c2k7ONv9ocOUH1icB/wA+CHw4e2uZxOP67cZ/cr2XeA75es8RuONtwP3A/8GHFeWD0Z3Dv0Q+B6juxG2/Tg2eOxnA7eW968EvgXsBb4I7CztR5bpvWX+K7e77imO9zeBhXKu/xk4ts/nGfgL4F7gLuAfgJ19PM/ADYw+V9jP6De0SzZyXoH3lOPfC7z7UGrw8QOS1EM1D8tIktZguEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ/8PYdNHPBsquAsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejecutamos nuestro algoritmo de optimización y graficamos la evolución de nuestra función de error por cada iteración\n",
    "errs = gd_mul(datos_mul, resultado_examen_enc)\n",
    "plt.plot(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO OPCIONAL:\n",
    "# 1) imprima el valor de los coeficientes\n",
    "# 2) imprima el ultimo valor del BCE\n",
    "# 3) Calcule la exactitud de todo el dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
