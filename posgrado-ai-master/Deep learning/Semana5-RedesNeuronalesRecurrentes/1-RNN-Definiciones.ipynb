{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a RNN\n",
    "\n",
    "Esta semana estudiaremos otra arquitectura de redes neuronales: RNN o Recurrent Neural Networks. Este tipo de redes son perfectas para analizar series temporales y secuencias de todo tipo de diferentes longitudes, a diferencia de otras arquitecturas como CNN, donde todos los tensores deben ser del mismo tamaño. \n",
    "\n",
    "Las redes RNN pueden servir para:\n",
    "\n",
    "- Anticipar trayectorias en vehículos autónomos.\n",
    "- Predecir precios de acciones.\n",
    "- Resolver problemas de conversión de voz a texto y NLP.\n",
    "- Reconocer lenguages de senas.\n",
    "- Procesar sequencias largas de audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronas Recurrentes y sus Capas\n",
    "\n",
    "Hasta el momento hemos trabajado con redes neuronales tipo “feed-forward” donde las conexiones en las neuronas van en un solo sentido, como es el caso de logistic regression o perceptron y CNN.\n",
    "\n",
    "Las redes neuronales recurrentes son muy similares a las arquitecturas ya vistas en nuestro curso con la salvedad de que ahora vamos a poder tener una conexión de una neurona consigo misma o bien con otra neurona contigua. \n",
    "\n",
    "Veamos el siguiente diagrama que representa este tipo de conexiones nuevas. \n",
    "\n",
    "<img src=\"img/rnn1.png\" />\n",
    "\n",
    "De acuerdo al diagrama, cada neurona recibe 2 inputs: el valor de $X_{(t)}$ para el frame t, y el valor de salida de la neurona anterior $y_{(t-1)}$.\n",
    "\n",
    "También, la neurona ahora tiene dos sets de pesos: los que corresponden a $x_{(t)}$ y los de la salida de $y_{(t-1)}$. Llamaremos a estos pesos $w_x$ y $w_y$. \n",
    "\n",
    "Salida de una neurona: \n",
    "- $y_{(t)} = \\phi({W_x}^Tx_{(t)}+{W_y}^Ty_{(t-1)}+b)$\n",
    "\n",
    "Salida de una capa de RNN de todas las instancias (vectorizado):\n",
    "- $y_{(t)} = \\phi(X_{(t)}{W_x}+y_{(t-1)}{W_y}+b)$\n",
    "- $y_{(t)} = \\phi([X_{(t)}y_{(t-1)}]W+b)$\n",
    "donde:\n",
    "- $W=[W_x W_y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celdas de Memoria (Memory Cells)\n",
    "\n",
    "Se llama celda, a la parte de una red neuronal que preserva el estado a través del tiempo. Usualmente una celda puede almacenar datos de 10 neuronas anteriores, lo que es conveniente para operaciones que ocupan una noción de los estados anteriores. \n",
    "\n",
    "Existen modificaciones que permiten aumentar la cantidad de memoria que se puede almacenar.\n",
    "\n",
    "El estado de una celda en el tiempo t se denota como ht. La letra h significa “hidden”. Ahora este estado h en neuronal simples es equivalente a la salida $y_t$, sin embargo en implementaciones mas complejas esto no es siempre así, por tanto vamos a describir como $h(t)=f(h_{(t-1)},x_{(t)})$.\n",
    "\n",
    "<img src=\"img/rnn2.png\" />\n",
    "\n",
    "*Notas importantes*\n",
    "\n",
    "- Las RNN reciben una sequencia y pruden producir otra sequencia. Esto sirve para generar predicciones en series temporales tipo acciones de la bolsa de valores o bien predecir la direccion de un vehiculo autonomo. (sequence-2-sequence)\n",
    "- Las RNN reciben una sequencia y pueden ignorar toda la sequencia con excepcion del ultimo elemento. Esto puede servir para predecir la proxima palabra de un texto. (sequence-2-vector)\n",
    "\n",
    "Con esta introducción, estamos listos para usar keras para crear nuestra primera red neuronal recurrente. Vamos al siguiente notebook para explorar la definición."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
