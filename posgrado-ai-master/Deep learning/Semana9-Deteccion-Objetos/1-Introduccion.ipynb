{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de Objetos\n",
    "\n",
    "Esta semana exploraremos algunas técnicas utilizadas para la detección de objetos en fotografías o videos.\n",
    "\n",
    "La detección de objetos es una tecnología informática relacionada con la visión por computadora y el procesamiento de imágenes que se ocupa de detectar instancias de objetos semánticos de una determinada clase (como humanos, edificios o automóviles) en imágenes y videos digitales.\n",
    "\n",
    "A diferencia de la clasificación de fotografías como lo hemos hecho anteriormente en este curso, ahora daremos un paso adicional al tratar de encontrar el objeto de interés en una imagen. Pueda que exista uno o varios objetos, por tanto, se deben realizar algunas modificaciones a las arquitecturas previamente estudiadas para soportar dicho requerimiento. \n",
    "\n",
    "<img src=\"img/od1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tecnicas Comunes \n",
    "\n",
    "Hoy en día es común asociar el reconocimiento de objetos con Deep Learning por la gran capacidad para clasificar imágenes y elementos de alta complejidad. Lo cierto es que esta subdisciplina a sufrido una evolución donde han surgido gran cantidad de métodos, no todos rápidos, ni eficientes.  \n",
    "Algunos de los métodos más populares son:\n",
    "\n",
    "-\tDetección con Cascadas Haar (AdaBoost) – Viola & Jones (2001)\n",
    "-\tHistograma de Gradientes (HOG) – (1986-2005) – McConnell (1986), Dalal (2005)\n",
    "-\tPropuesta de Regiones\n",
    "    -\tR-CNN  - Girshick (2014)\n",
    "    -\tFast R-CNN - - Girshick (2015)\n",
    "    -\tFaster R-CNN - Shaoqing (2014)\n",
    "    -\tMask R-CNN – Girshick et al (2017)\n",
    "    -\tCascade R-CNN – Jiangmiao et al, (2019)\n",
    "-\tSingle-Shot Multibox Detector (SSD) – Liu (2016)\n",
    "-\tYOLO – Redmon (2016, 2017, 2018) v1,v2,v3 – Bochkovskiy (2020) v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificacion, Deteccion y Segmentacion\n",
    "\n",
    "La segmentación es un tipo de etiquetado en el que cada píxel de una imagen se etiqueta con conceptos determinados. Aquí, las imágenes completas se dividen en grupos de píxeles que luego se pueden etiquetar y clasificar, con el objetivo de simplificar una imagen o cambiar la forma en que se presenta una imagen al modelo, para que sea más fácil de analizar.\n",
    "\n",
    "Los modelos de segmentación proporcionan el contorno exacto del objeto dentro de una imagen. Es decir, se proporcionan detalles píxel a píxel para un objeto dado, a diferencia de los modelos de clasificación, donde el modelo identifica lo que hay en una imagen, y los modelos de detección, que coloca un cuadro delimitador alrededor de objetos específicos.\n",
    "\n",
    "<img src=\"img/od2.jpeg\" />\n",
    "\n",
    "### Region-based R-CNN\n",
    "\n",
    "Premisas:\n",
    "\n",
    "- un solo objeto de interes puede estar en una determinada region\n",
    "- Las regiones propuestas son generadas por un algoritmo llamado **Selective Search**\n",
    "- internamente tiene 2 redes neuronales convolucionales.Uno para los bounding boxes y otro para las clases.\n",
    "\n",
    "Selective Search:\n",
    "\n",
    "- 1) se generan varias regiones alrededor de la imagen.\n",
    "- 2) se combinan pares de regiones basadas en similitudes.\n",
    "- 3) esto se repite hasta que quede solo una region... o muy pocas\n",
    "\n",
    "<img src=\"img/selective-search.png\" />\n",
    "\n",
    "- 4) de las regiones generadas se generan una lista de posibles ventanas de clasificacion. \n",
    "\n",
    "#### Propuestas de Regiones\n",
    "\n",
    "- RCNN genera cerca de 2000 regiones por foto.\n",
    "- Se mueve una ventana de a traves de las regiones para identificar el objeto. Se extraen features de la imagen con SVM\n",
    "- Se utiliza IoU para evaluar las ventanas e identificar un objeto. Recordemos que un objeto solo puede estar una vez en un bounding box, entonces se selecciona el mejor IoU.\n",
    "\n",
    "<img src=\"img/iou.png\" />\n",
    "\n",
    "\n",
    "### Fast R-CNN\n",
    "\n",
    "\n",
    "- Fast R-CNN usa solo una CNN para todos los features.\n",
    "- Se sigue utilizando selective search para las regiones de interes (RoI)\n",
    "- El RoI es una capa de pooling espacial piramidal.\n",
    "- al final, la Fast  R-CNN se bifurca en 2 salidas: una con un softmax para las clases y otra con un regresor para los bounding boxes.\n",
    "- ya no se usa SVM para la clasificacion de objetos, sino que se usa softmax.\n",
    "- Estos cambios permitieron a Fast R-CNN ser mucho mas rapida para clasificar que R-CNN.\n",
    "\n",
    "<img src=\"img/fast.png\" />\n",
    "\n",
    "\n",
    "### Faster R-CNN\n",
    "\n",
    "- Se elimina Selective Search porque es costoso. \n",
    "- Se usa Region Proposal Network (RPN); otro algoritmo para generar regiones de forma mas eficiente.\n",
    "- El resto de la arquitectura es igual a Fast R-CNN. \n",
    "- Basicamente la ganancia fue en el cambio por RPN.\n",
    "\n",
    "<img src=\"img/faster.png\" />\n",
    "\n",
    "#### RPN:\n",
    "\n",
    "Region Proposal Network toma una imagen de cualquier tamaño como entrada y genera un conjunto de propuestas de objetos rectangulares, cada una con una puntuación. Lo hace deslizando una pequeña red sobre el mapa de características generado por la capa convolucional.\n",
    "\n",
    "Le features a partir de RPN se alimenta a dos capas hermanas completamente conectadas: una capa de regresión de cuadro para el cuadro delimitador y una capa de clasificación de cuadro para la clasificación de objetos.\n",
    "\n",
    "Los **Arnchors** son los tamanos de los square features que se usan para evaluar el contenido de la ventana que se mueve (sliding window).\n",
    "\n",
    "<img src=\"img/anchors.png\" />\n",
    "\n",
    "\n",
    "\n",
    "### Yolo v3:\n",
    "\n",
    "<img src=\"img/yolov3.png\" />\n",
    "\n",
    "Yolo aplica una única red neuronal a la imagen completa. Esta red divide la imagen en regiones y predice cuadros delimitadores y probabilidades para cada región. Estos cuadros delimitadores están ponderados por las probabilidades predichas.\n",
    "\n",
    "El modelo tiene varias ventajas sobre los sistemas basados en clasificadores. Mira la imagen completa en el momento de la prueba, por lo que sus predicciones se basan en el contexto global de la imagen. También hace predicciones con una única evaluación de red, a diferencia de sistemas como R-CNN, que requieren miles para una sola imagen. Esto lo hace extremadamente rápido, más de 1000 veces más rápido que R-CNN y 100 veces más rápido que Fast R-CNN. \n",
    "\n",
    "Caracteristicas\n",
    "\n",
    "- genera un grid por toda la imagen. Este grid es el equivalente a las regiones. por tanto usa menos regiones que la mayoria de los algoritmos de deteccion.\n",
    "- cada uno de las regiones solamente puede ser asignado a una clase.\n",
    "- Yolo v3 esta compuesto por 106 capas.\n",
    "- Al igual que RetinaNet, Yolo es un one-shot detector.\n",
    "\n",
    "<img src=\"img/yolomap.jpg\" />\n",
    "\n",
    "### Metricas\n",
    "\n",
    "#### IoU: Intersection over union (Ap-IoU)\n",
    "\n",
    "IoU mide la superposición entre 2 límites. Usamos eso para medir cuánto se superpone nuestro límite predicho con la verdad fundamental (el límite del objeto real). En algunos conjuntos de datos, predefinimos un umbral de IoU (digamos 0,5) para clasificar si la predicción es un verdadero positivo o un falso positivo.\n",
    "\n",
    "\n",
    "#### mAP: mean Average Precision \n",
    "\n",
    "AP (precisión promedio) es una métrica popular para medir la precisión de detectores de objetos como Faster R-CNN, SSD, etc. La precisión promedio calcula el valor de precisión promedio para el valor de Recall de 0 a 1.\n",
    "\n",
    "Recall: TP / (TP+FN)\n",
    "\n",
    "\n",
    "### Velocidad de los detectores de objetos\n",
    "\n",
    "<img src=\"img/yolo.png\" />\n",
    "\n",
    "<img src=\"img/yolot.png\" />\n",
    "\n",
    "Yolo v4: https://arxiv.org/pdf/2004.10934.pdf\n",
    "\n",
    "\n",
    "### Formatos de Etiquetado & Herramientas\n",
    "\n",
    "#### PASCAL VOC XML\n",
    "\n",
    "http://host.robots.ox.ac.uk/pascal/VOC/#:~:text=The%20PASCAL%20VOC%20project%3A,2005%2D2012%2C%20now%20finished)\n",
    "\n",
    "<img src=\"img/voc.png\" />\n",
    "\n",
    "herramientas:\n",
    "\n",
    "- https://github.com/manhcuogntin4/Label-Annotation-VOC-Pascal\n",
    "- https://github.com/wkentaro/labelme\n",
    "- https://rectlabel.com/\n",
    "\n",
    "### COCO Labels\n",
    "\n",
    "https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch\n",
    "https://cocodataset.org/#format-data\n",
    "\n",
    "<img src=\"img/coco.png\" />\n",
    "\n",
    "herramientas:\n",
    "\n",
    "- https://github.com/wkentaro/labelme\n",
    "- https://github.com/jsbroks/coco-annotator\n",
    "- https://github.com/visipedia/annotation_tools\n",
    "\n",
    "### YOLO Labels\n",
    "\n",
    "<img src=\"img/yoloa.png\" />\n",
    "\n",
    "herramientas:\n",
    "\n",
    "- https://github.com/tzutalin/labelImg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
