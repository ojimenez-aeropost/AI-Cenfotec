{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wqNOUrokTxL-"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Este notebook require Tensorflow 2.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPC8GPMcUIb7"
   },
   "source": [
    "### Descargar Extracto der Shakespeare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXNC2B_WUG04",
    "outputId": "3cc0e73c-5aa4-4934-aeb3-154cd758832f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1WkbSWCU16z"
   },
   "source": [
    "### Set de caracteres en la obra (FYI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9bW4N1uBUr9L",
    "outputId": "abc708ea-3c6e-463e-b7ad-aa2b1598a88f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9K3q1neVS4L"
   },
   "source": [
    "### Tokenizacion del texto con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMWvdkR3UuUZ",
    "outputId": "53de2c8f-b601-4d84-d404-04f382f392a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 23, 2, 5, 25]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "\n",
    "# probamos la palabra\n",
    "tokenizer.texts_to_sequences([\"Speak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEwjsUmlVvR4",
    "outputId": "1699620a-7663-4f8f-90c0-c58cd74e5040"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s p e a k']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos converir esa secuencia en texto:\n",
    "tokenizer.sequences_to_texts([[8, 23, 2, 5, 25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVtYEw_RV_9A",
    "outputId": "b7d2eaf5-58d8-4ea3-d06c-3873ae500d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos distintos 39\n",
      "Cantidad de elementos 1115394\n"
     ]
    }
   ],
   "source": [
    "# numero de caracteres distintos\n",
    "max_id = len(tokenizer.word_index) \n",
    "\n",
    "# cantidad total del caracteres\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "print(\"Elementos distintos\",max_id)\n",
    "print(\"Cantidad de elementos\",dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w-gdKXJWZmC"
   },
   "source": [
    "### Creamos el Train-Set usando las Funciones de NLP (ver Notebook #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_20jThaNWpt2"
   },
   "outputs": [],
   "source": [
    "# se convierte TODO el texto en secuencias.\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "\n",
    "# se genera un train-set del 90% de las secuencias\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L7slhrCAXCY3"
   },
   "outputs": [],
   "source": [
    "# se define que las cadenas son de 100 caracteres con 1 caracter de shift\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # tamano de la ventana\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# se generan las cadenas planas\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iI9KSwhwXT3c",
    "outputId": "bc58d6df-cecc-4502-b90a-086f97f6f2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# generamos el x_train y y_train usando el mismo codigo del notebook #1\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "batch_size = 32\n",
    "\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda x_train, y_train: (tf.one_hot(x_train, depth=max_id), y_train))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# se revisan los tamanos de los tensores generados\n",
    "for x, y in dataset.take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ft8R4BwYfRi"
   },
   "source": [
    "### Modelo Stateful RNN-GRU y Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyJNENzhYnK6",
    "outputId": "4bcf0ab0-b49b-45c1-c93a-21afffe7046a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 313 steps\n",
      "Epoch 1/50\n",
      "313/313 [==============================] - 47s 149ms/step - loss: 2.6224\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 2.2280\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.1504\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.4703\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.3564\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 2.2239\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.0772\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.0791\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 2.0368\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.9600\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.9783\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.9186\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.9157\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.8920\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.8752\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.8398\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.8130\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.7636\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 1.7481\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.7215\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.7060\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6912\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6804\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6699\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6603\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.6537\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.6460\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.6397\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6339\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6293\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6227\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6182\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6163\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6103\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.6069\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.6045\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 1.6017\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5973\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5935\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5924\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5899\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5882\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.5833\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5835\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5788\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5779\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5753\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5739\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5725\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 1.5713\n"
     ]
    }
   ],
   "source": [
    "# arquitecturta\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# compilacion y entrenamiento\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es un super hack!\n",
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stateless_model\n",
    "\n",
    "# Guardar el Modelo\n",
    "model.save('shakespeare-stateful.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2-TinyShakespeare",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
