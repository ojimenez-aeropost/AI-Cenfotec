{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation - Errores\n",
    "\n",
    "En la sección anterior de “forward propagation” vimos como las funciones de activación son capaces de pasar un mensaje de la capa de input, hasta la última capa de output a través de las funciones de activación en cada capa oculta. También notamos la facilidad y eficiencia con la que esto se ejecuta mediante vectorización.\n",
    "\n",
    "Ahora, al igual que en el algoritmo de Gradient Descent visto con Logistic Regression, debemos estimar el error de nuestros cálculos; es decir, la diferencia entre el output de la red neuronal y la etiqueta (label) para ese sample en específico.\n",
    "\n",
    "La idea del algoritmo de backpropagation es que el error debe estimarse en cada capa (hidden layer) para poder determinar la contribución de cada peso w en el error de el nodo que apunta. \n",
    "\n",
    "Los errores en una red neuronal son acumulativos, es decir, el error de la capa de salida “va hacia atras” afectandos el cálculo de los otros errores.\n",
    "\n",
    "En este notebook, solamente vamos a calcular los errores en cada capa. Mas adelante comprenderemos como re-calcular los pesos basados en gradientes.   \n",
    "\n",
    "\n",
    "<img src=\"img/bp1.png\" />\n",
    "\n",
    "Para calcular el error debemos realizar los siguientes pasos primero:\n",
    "- Eoutput1 (“e1”): es el error de la capa de salida. Este es simplemente la diferencia de y & y’.\n",
    "- Eh1: para el nodo “verde” debemos calcular la contribución del error que causo el peso w11. \n",
    "- Eh2: de forma similar a Eh1, calculamos la contribución del error que causo w21.\n",
    "\n",
    "Ahora, el error (e1) es la suma de las proporciones calculadas anteriormente para cada nodo. Por tanto, el error (e1) es la suma de los errores eh de sus salidas multiplicado por el error calculado de la capa anterior. Este proceso se repite en cada capa hasta llegar al primer “hidden layer”. Con este proceso podemos calcular todos los errores de cada capa.\n",
    "En el siguiente diagrama podemos ver los cálculos que deben realizarse para calcular el error en las distintas capas. \n",
    "\n",
    "<img src=\"img/bp2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de Errores con Vectorización \n",
    "\n",
    "**WARNING:** \n",
    "Para poder hacer la ejecución vectorizada, vamos a eliminar el denominador de las proporciones del error de cada peso, esto para poder ejecutar el cálculo de errores de la forma más eficiente con un simple producto de matrices (dot-product). Como el denominador es solo un elemento normalizador, el calculo del error es proporcional, pero diferente.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# errores de la capa de salida\n",
    "eo = np.array([[0.8,0.5]]).T\n",
    "\n",
    "print(eo)\n",
    "print(\"\")\n",
    "\n",
    "# pesos hidden layer 2\n",
    "wh2 = np.array([[2.0,1.0],[3.0, 4.0]])\n",
    "               \n",
    "print(wh)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Error Layer 2\")\n",
    "e_L2 = np.dot(wh2,eo)\n",
    "print(e_L2)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesos hidden layer 1\n",
    "wh1 = np.array([[3.0,1.0],[2.0, 7.0]])\n",
    "\n",
    "print(\"Error Layer 1\")\n",
    "e_L1 = np.dot(wh1,e_L2)\n",
    "print(e_L1)\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
