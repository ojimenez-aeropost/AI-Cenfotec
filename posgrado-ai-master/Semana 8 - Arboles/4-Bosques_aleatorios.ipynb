{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosques aleatorios\n",
    "\n",
    "Los árboles de decisión son un buen método de aprendizaje supervisado. Su **interpretabilidad** es de gran ayuda para ayudarnos a entender **por qué** toma una decisión de clasificación específica. Sin embargo, tienen una desventaja bastante seria: tienden a crear modelos con **bajo sesgo y alta varianza**. Esto lo que quiere decir es que, en general, **tienden a sobreajustarse** al dataset de entrenamiento y, por ende, su desempeño con un dataset de prueba puede volverse pobre.\n",
    "\n",
    "Veamos un ejemplo con el dataset de los exámenes, y comparemos los resultados de un árbol de decisión contra los de la regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas relevantes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cargamos el dataset de los exámenes\n",
    "horas_estudio = [5, 4, 1, 1, 1, 2, 3, 3, 2, 0, 3, 3, 2, 3, 2, 3, 1, 5, 4, 4, 3, 4, 2, 4, 3, 3, 6, 4, 4, 5, 3, 4, 3, 2, 6, 2, 4, 4, 1, 5, 3, 3, 5, 7, 4, 3, 5, 2, 3, 5, 3, 4, 3, 3, 1, 3, 6, 0, 4, 3, 7, 2, 5, 3, 3, 4, 3, 1, 5, 4, 1, 4, 3, 1, 2, 4, 2, 2, 2, 2, 4, 3, 3, 1, 4, 2, 1, 0, 4, 4, 3, 3, 4, 6, 5, 3, 2, 2, 6, 6]\n",
    "horas_sueno = [4, 4, 5, 5, 4, 6, 7, 7, 5, 8, 5, 7, 7, 7, 7, 7, 4, 8, 4, 8, 6, 8, 5, 4, 5, 5, 7, 4, 6, 8, 6, 4, 7, 4, 5, 7, 7, 4, 6, 6, 5, 6, 7, 6, 4, 8, 7, 4, 4, 6, 5, 8, 6, 8, 4, 5, 4, 6, 8, 5, 6, 6, 5, 6, 4, 8, 4, 6, 5, 7, 5, 6, 7, 7, 8, 4, 4, 6, 8, 6, 4, 6, 5, 5, 5, 4, 4, 7, 8, 5, 4, 8, 5, 6, 5, 6, 7, 7, 4, 5]\n",
    "resultado = np.array(['APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO'])\n",
    "\n",
    "# Creamos la matriz de diseño con las variables independientes\n",
    "datos_x = np.array((horas_estudio, horas_sueno)).transpose()\n",
    "\n",
    "# Entrenamos ambos modelos\n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "print('cross_val logistica =', cross_val_score(lr, np.array((horas_estudio, horas_sueno)).T, resultado, cv=5).mean())\n",
    "print('cross_val arbol = ', cross_val_score(dt, np.array((horas_estudio, horas_sueno)).T, resultado, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alto sesgo / baja varianza\n",
    "En este aspecto, los árboles de decisión son el **opuesto a la regresión logística sin regularización**, la cual tiene **alto sesgo pero baja varianza**. Al regularizar la regresión logística, aceptamos un incremento leve en la varianza para lograr una mayor reducción del sesgo.\n",
    "\n",
    "El *cómo hacer* ese \"intercambio\" de sesgo por varianza con modelos de árboles ha sido objeto de estudio por mucho tiempo. Varias soluciones parciales se han planteado y hoy son parte de las bibliotecas de entrenamiento tradicional, incluyendo ideas como:\n",
    "\n",
    "- Limitar la profundidad del árbol de decisión a un número máximo de niveles\n",
    "- Requerir un tamaño mínimo de partición para evaluar predicados\n",
    "- Requerir una cantidad mínima de ganancia de información para un predicado\n",
    "- \"Recortar\" (*pruning*) ciertas ramas del árbol resultante\n",
    "\n",
    "Estas soluciones tienden a mejorar la calidad de los modelos resultantes, pero no resuelven el problema de fondo en su totalidad.\n",
    "\n",
    "Hoy estudiaremos dos **técnicas generales** que, aunque son aplicables a cualquier modelo de aprendizaje supervisado, tienen excelentes resultados al aplicarse a árboles de decisión. De hecho, tienden a producir clasificadores de **altísima calidad**, rara vez superados por otros métodos. La unión de estas técnicas la conocemos como un **bosque aleatorio** (*random forest* en inglés):<br>&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "<center><b>Random Forest = Bootstrap aggregation + Random subspaces</b></center>\n",
    "\n",
    "## Ingrediente 1: agregación bootstrap\n",
    "\n",
    "Mucho se ha investigado sobre **cómo combinar distintos modelos en uno solo**. Por ejemplo, si tenemos un dataset $D$, podemos entrenar con este distintos modelos de regresión logística, de árboles de decisión, vecinos cercanos, redes neuronales, SVMs, etc. y queremos combinarlos para producir un **modelo de conjunto** o *ensemble* con la expectativa de que **produzca mejores resultados** que los métodos individuales por separado.\n",
    "\n",
    "Los métodos de modelos de conjunto o *ensemble* plantean soluciones a las dos siguientes preguntas:\n",
    "\n",
    "1. ¿Cómo entreno cada modelo individual para que aporte lo mejor de sí al conjunto? \n",
    "\n",
    "2. ¿Cómo combino las predicciones de mis modelos en una sola predicción final?\n",
    "\n",
    "Respondamos primero la pregunta (2): si ya tenemos los $K$ modelos entrenados, podemos simplemente ponerlos a **votar** y producir como salida la clase mayoritaria (en el caso de clasificación), o tomar **la media** de las salidas (en el caso de regresión).\n",
    "\n",
    "La respuesta a la pregunta (1) es más delicada. Si entrenamos $K$ árboles de decisión sobre el mismo datset $D$, obtendremos **el mismo árbol de decisión repetido** $K$ **veces**, lo cual no aportaría absolutamente nada.\n",
    "\n",
    "La solución a esto es utilizar **el bootstrap**, una técnica estadística de **resampling** que se utiliza para evaluar la calidad de un estimador. El procedimiento es el siguiente: tenemos un dataset $D$ de $N$ filas y queremos entrenar $K$ árboles de decisión distintos, entonces generaremos $K$ \"datasets aleatorios\" $\\hat{D}_1, \\hat{D}_2, ..., \\hat{D}_K$ cada uno de tamaño $N$, en el que las filas son **muestreadas con reemplazo**. Esto lo que quiere decir es que la fila $D_i$ podría aparecer en $\\hat{D}_1$ pero no en $\\hat{D}_2$ o podría aparecer en $\\hat{D}_3$ **más de una vez**. \n",
    "\n",
    "Veamos un ejemplo de como se vería **el bootstrap** en un pequeño dataset de seis filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from IPython.display import display\n",
    "x = pd.DataFrame(['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "\n",
    "print('Dataset original: ')\n",
    "display(x)\n",
    "\n",
    "print('Bootstrap 1: ')\n",
    "display(resample(x))\n",
    "\n",
    "print('Bootstrap 2: ')\n",
    "display(resample(x))\n",
    "\n",
    "print('Bootstrap 3: ')\n",
    "display(resample(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, una vez hemos generado los datasets aleatorios $\\hat{D}_1, \\hat{D}_2, ..., \\hat{D}_K$, procedemos entonces a entrenar $K$ modelos distintos, uno con cada dataset aleatorio. \n",
    "\n",
    "\n",
    "## Ingrediente 2: subespacios aleatorios\n",
    "\n",
    "El segundo ingrediente en los bosques aleatorios es el de los subespacios aleatorios. Esta técnica la volveremos a ver cuando estudiemos regularización en redes neuronales bajo el nombre de **dropout**. \n",
    "\n",
    "La idea es parecida a la anterior: para cada uno de los datasets aleatorios $\\hat{D}_1, \\hat{D}_2, ..., \\hat{D}_K$, vamos a seleccionar un subconjunto pequeño de columnas al azar, de modo que el modelo correspondiente no utilice todas las columnas posibles, sino solamente las que le fueron asignadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = np.array(['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "\n",
    "print('Conjunto de columnas original: ', cols)\n",
    "print('Subespacio aleatorio 1: ', np.random.choice(cols, 3, replace=False))\n",
    "print('Subespacio aleatorio 2: ', np.random.choice(cols, 3, replace=False))\n",
    "print('Subespacio aleatorio 3: ', np.random.choice(cols, 3, replace=False))\n",
    "print('Subespacio aleatorio 4: ', np.random.choice(cols, 3, replace=False))\n",
    "print('Subespacio aleatorio 5: ', np.random.choice(cols, 3, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bosques aleatorios en ``scikit-learn`` \n",
    "\n",
    "Las clases que debemos utilizar son ``sklearn.ensemble.RandomForestClassifier`` y ``sklearn.ensemble.RandomForestRegressor``. \n",
    "\n",
    "Veámoslas en acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfc = RandomForestClassifier().fit(datos_x, resultado)\n",
    "\n",
    "print('cross_val bosque = ', cross_val_score(RandomForestClassifier(), np.array((horas_estudio, horas_sueno)).T, resultado, cv=5).mean())\n",
    "\n",
    "XX, YY = np.meshgrid(np.linspace(0, 10, 50), np.linspace(0, 10, 50))\n",
    "pos = np.vstack([XX.ravel(), YY.ravel()])\n",
    "pred = ['green' if x == 'APROBADO' else 'red' for x in rfc.predict(pos.T) ]\n",
    "plt.scatter(pos[0], pos[1], c=pred)\n",
    "plt.title('Dataset de examenes')\n",
    "plt.show()\n",
    "\n",
    "estatura = np.array([159, 177, 160, 181, 172, 149, 160, 182, 170, 153, 181, 171, 190, 181, 164, 185, 163, 167, 176, 145, 178, 182, 170, 186, 164, 153, 177, 153, 155, 168, 182, 173, 166, 154, 179, 179, 174, 172, 170, 171, 176, 183, 160, 175, 173, 162, 174, 173, 170, 173, 176, 171, 159, 179, 148, 161, 194, 174, 169, 179, 185, 176, 172, 171, 171, 150, 179, 153, 166, 177, 178, 172, 154, 179, 155, 156, 149, 181, 159, 161, 177, 173, 170, 181, 161, 164, 171, 178, 158, 145, 156, 164, 174, 159, 167, 171, 182, 179, 166, 161, 164, 176, 183, 168, 169, 173, 166, 176, 170, 173, 173, 169, 161, 163, 170, 161, 184, 164, 179, 176, 181, 173, 185, 167, 174, 168, 183, 170, 172, 158, 188, 171, 173, 156, 184, 158, 161, 167, 150, 159, 180, 171, 162, 174, 167, 177, 172, 163, 169, 168, 174, 179, 171, 191, 166, 163, 167, 181, 175, 177, 157, 176, 168, 178, 175, 161, 179, 169, 167, 155, 148, 171, 189, 174, 188, 181, 181, 156, 175, 169, 156, 168, 165, 163, 182, 177, 162, 180, 183, 158, 165, 164, 170, 172, 168, 161, 183, 186, 191, 168]).reshape(-1, 1)\n",
    "peso = np.array([44, 81, 64, 84, 58, 52, 61, 90, 78, 53, 85, 77, 101, 83, 70, 85, 75, 83, 69, 28, 79, 88, 75, 93, 76, 54, 77, 50, 73, 73, 72, 78, 62, 74, 70, 91, 72, 73, 78, 63, 75, 67, 71, 83, 61, 65, 82, 83, 82, 71, 85, 100, 82, 76, 48, 53, 97, 76, 70, 76, 78, 69, 56, 82, 82, 65, 87, 40, 61, 69, 59, 68, 41, 70, 52, 49, 44, 74, 56, 62, 90, 79, 61, 79, 59, 70, 75, 100, 67, 40, 50, 59, 73, 49, 79, 86, 77, 73, 58, 53, 57, 87, 78, 66, 77, 72, 64, 76, 65, 96, 73, 50, 54, 64, 57, 54, 78, 60, 86, 65, 77, 89, 83, 75, 92, 60, 70, 77, 77, 64, 93, 60, 74, 37, 96, 46, 79, 78, 51, 37, 86, 59, 67, 92, 64, 72, 82, 68, 49, 74, 73, 71, 82, 78, 56, 73, 69, 73, 81, 88, 47, 79, 61, 67, 71, 57, 68, 64, 55, 66, 54, 93, 78, 76, 97, 98, 52, 79, 93, 62, 53, 69, 73, 59, 76, 81, 74, 73, 72, 77, 74, 76, 63, 83, 63, 51, 96, 81, 87, 80])\n",
    "rfr = RandomForestRegressor().fit(estatura, peso)\n",
    "\n",
    "vx = np.array(np.linspace(145,195,200)).reshape(-1,1)\n",
    "plt.scatter(estatura,peso,c='lightgray')\n",
    "plt.title('Dataset de peso/estatura')\n",
    "plt.plot(vx, rfr.predict(vx), c='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
