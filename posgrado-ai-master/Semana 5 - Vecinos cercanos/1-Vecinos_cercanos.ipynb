{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vecinos cercanos\n",
    "\n",
    "Hoy vamos a estudiar uno de los métodos más simples, poderosos e interesantes del arsenal de técnicas de machine learning: el método de los ***k* vecinos más cercanos**. \n",
    "\n",
    "Para motivar este método, recordemos el famoso adagio \"*si camina como pato, suena como pato y nada como pato... muy posiblemente* ***es un pato***.\"\n",
    "\n",
    "Retomemos nuestro dataset de **aprobados y reprobados**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutemos esta celda para cargar el dataset y las bibliotecas básicas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "horas_estudio = [5.25, 4.5, 1.75, 1.75, 1.5, 2.75, 3.75, 3.75, 2.75, 0.75, 3.5, 3.75, 2.5, 3.75, 2.75, 3.25, 1.5, 5.5, 4.5, 4.75, 3.25, 4.5, 2.5, 4.25, 3.75, 3.75, 6.5, 4.25, 4.25, 5.5, 3.75, 4.5, 3.5, 2.25, 6.25, 2.75, 4.75, 4.5, 1.75, 5.25, 3.25, 3.5, 5.25, 7.5, 4.25, 3.25, 5.25, 2.75, 3.25, 5.5, 3.75, 4.5, 3.25, 3.75, 1.25, 3.25, 6.25, 0.5, 4.5, 3.75, 7.25, 2.75, 5.75, 3.75, 3.75, 4.5, 3.25, 1.25, 5.5, 4.5, 1.5, 4.75, 3.5, 1.25, 2.5, 4.75, 2.25, 2.5, 2.25, 2.75, 4.75, 3.25, 3.25, 1.25, 4.25, 2.75, 1.25, 0.25, 4.75, 4.75, 3.75, 3.75, 4.75, 6.5, 5.25, 3.25, 2.25, 2.25, 6.25, 6.5]\n",
    "horas_sueno = [4.75, 4.25, 5.5, 5.25, 4.5, 6.25, 7.5, 7.75, 5.25, 8.5, 5.75, 7.5, 7.75, 7.25, 7.5, 7.25, 4.25, 8.75, 4.75, 8.25, 6.75, 8.25, 5.75, 4.25, 5.25, 5.25, 7.75, 4.75, 6.5, 8.25, 6.5, 4.5, 7.75, 4.75, 5.5, 7.75, 7.25, 4.25, 6.75, 6.25, 5.75, 6.75, 7.25, 6.25, 4.5, 8.5, 7.5, 4.25, 4.5, 6.5, 5.5, 8.75, 6.25, 8.5, 4.5, 5.75, 4.5, 6.75, 8.5, 5.5, 6.75, 6.25, 5.25, 6.25, 4.5, 8.5, 4.75, 6.25, 5.25, 7.25, 5.25, 6.75, 7.25, 7.25, 8.5, 4.25, 4.75, 6.5, 8.25, 6.5, 4.5, 6.75, 5.75, 5.5, 5.5, 4.5, 4.5, 7.25, 8.25, 5.75, 4.75, 8.25, 5.25, 6.25, 5.5, 6.75, 7.75, 7.75, 4.25, 5.25]\n",
    "resultado_examen = ['APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'APROBADO', 'REPROBADO', 'REPROBADO', 'APROBADO', 'APROBADO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bosquejo del método *k*-NN\n",
    "\n",
    "La intuición es muy simple. Dada un punto $\\mathbf{x}$ que queremos clasificar y un dataset $D$, buscaremos los $k$ puntos **más parecidas** a $\\mathbf{x}$ en $D$ y clasificaremos a $\\mathbf{x}$ de acuerdo a la **etiqueta mayoritaria**. \n",
    "\n",
    "Para explicar esto mejor, grafiquemos nuestro dataset de aprobados y reprobados, los cuales se muestran en verde y rojo, respectivamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(horas_estudio, horas_sueno, c=['lightgreen' if x == 'APROBADO' else 'red' for x in resultado_examen], s=50)\n",
    "plt.xlabel(\"horas_estudio\")\n",
    "plt.ylabel(\"horas_sueno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, supongamos que vamos a clasificar a un individuo que estudió 3 horas y durmió 6 horas, utilizando el método *k-NN* donde el valor de $k=3$. El valor a clasificar está representado con una estrella negra y los tres vecinos más cercanos están indicados con un circulo alrededor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(horas_estudio, horas_sueno, c=['lightgreen' if x == 'APROBADO' else 'red' for x in resultado_examen], s=50)\n",
    "plt.scatter(3, 6, c='black', marker='*', s=100)\n",
    "plt.scatter(2.75, 6.25, marker='o', facecolors='none', edgecolors='blue', s=200)\n",
    "plt.scatter(3.25, 5.75, marker='o', facecolors='none', edgecolors='blue', s=200)\n",
    "plt.scatter(3.25, 6.25, marker='o', facecolors='none', edgecolors='blue', s=200)\n",
    "plt.xlabel(\"horas_estudio\")\n",
    "plt.ylabel(\"horas_sueno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los tres vecinos más cercanos al punto que queremos clasificar cuentan dos aprobados (verdes) y un reprobado (rojo). Por ende, clasificamos a este punto como **aprobado**. \n",
    "\n",
    "### Ingrediente: la función de distancia\n",
    "\n",
    "Hasta ahora no hemos dado una definición clara de lo que significa un vecino **\"cercano\"**. Simplemente nos orientamos visualmente. Sin embargo, nuestro método requiere que utilicemos una **función de distancia** de la forma $\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right)$, la cual devuelve un número positivo que mide **qué tan distintos son** los vectores $\\mathbf{a}$ y $\\mathbf{b}$, siendo 0 indicativo de que no hay ninguna diferencia entre ellos, y mientras más alto sea más distintos son. \n",
    "\n",
    "La función $\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right)$ debe ser una **métrica**. Una métrica es una función que posee las siguientes características:\n",
    "\n",
    "1. No-negatividad: $\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) \\ge 0$\n",
    "2. Simetría: $\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) = \\vec{d}\\left(\\mathbf{b},\\mathbf{a}\\right)$\n",
    "3. Identidad de los indiscernibles: $\\vec{d}\\left(\\mathbf{a},\\mathbf{a}\\right) = 0$\n",
    "4. Desigualdad de triangulos: $\\vec{d}\\left(\\mathbf{a},\\mathbf{c}\\right) \\le \\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right)  + \\vec{d}\\left(\\mathbf{b},\\mathbf{c}\\right)$\n",
    "\n",
    "Una de las funciones de distancia más comunmente utilizadas es la **distancia euclideana**, también conocida como **norma** $L_2$, la cual está definida como $$\\vec{d}_{\\mathrm{Euclideana}}\\left(\\mathbf{a},\\mathbf{b}\\right) = \\sqrt{\\sum_{i=i}^{M} (a_i - b_i)^2}$$\n",
    "Posteriormente veremos otras funciones de distancia.\n",
    "\n",
    "Implementemos una distancia euclideana para nuestro ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distancia euclideana para nuestro dataset de ejemplo\n",
    "# Los puntos se definen como tuplas de la forma (horas_estudio, horas_sueno)\n",
    "def distancia(punto_a, punto_b):\n",
    "    (horas_estudio_a, horas_sueno_a) = punto_a\n",
    "    (horas_estudio_b, horas_sueno_b) = punto_b\n",
    "    \n",
    "    return np.sqrt((horas_estudio_a - horas_estudio_b)**2 + (horas_sueno_a - horas_sueno_b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probemos nuestra función de distancia con algunos valores\n",
    "punto_x = (3,4)\n",
    "punto_y = (5,6)\n",
    "punto_z = (8,9)\n",
    "\n",
    "print('distancia(punto_x, punto_x) =', distancia(punto_x, punto_x))\n",
    "print('distancia(punto_x, punto_y) =', distancia(punto_x, punto_y))\n",
    "print('distancia(punto_y, punto_x) =', distancia(punto_y, punto_x))\n",
    "print('distancia(punto_y, punto_z) =', distancia(punto_y, punto_z))\n",
    "print('distancia(punto_x, punto_z) =', distancia(punto_x, punto_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos crear nuestra función de clasificación ``es_aprobado``. Implementaremos una versión \"ingénua\" o simplificada del método k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocamos el dataset como una lista de tuplas de la forma (horas_estudio, horas_sueno, resultado_examen)\n",
    "todo_dataset = list(zip(horas_estudio, horas_sueno, resultado_examen))\n",
    "\n",
    "# Definimos nuestra función de clasificación\n",
    "def es_aprobado(mis_horas_estudio, mis_horas_sueno, k=3, datos=todo_dataset):\n",
    "    \n",
    "    mi_punto = (mis_horas_estudio, mis_horas_sueno)\n",
    "    \n",
    "    # Primero, calculamos la distancia de nuestro punto contra cada otro punto en el dataset\n",
    "    # El resultado de esta operación es una lista de tuplas de la forma (distancia_hasta_x, resultado_x)\n",
    "    todas_distancias = [ (distancia(mi_punto, (estudio_x, sueno_x)), label_x) for (estudio_x, sueno_x, label_x) in datos ]\n",
    "\n",
    "    # Ahora, ordenamos la lista por distancia\n",
    "    todas_distancias.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Tomamos las etiquetas de los k valores más cercanos\n",
    "    k_etiquetas = [x[1] for x in todas_distancias[:k]]\n",
    "    \n",
    "    # Contamos las veces que aparecen\n",
    "    # Esta operación devuelve una lista de la forma (resultado_examen, numero_vecinos)\n",
    "    cuentas = ([ (x, k_etiquetas.count(x)) for x in set(k_etiquetas) ])\n",
    "    \n",
    "    # Y encontramos el máximo\n",
    "    resultado = max(cuentas, key=lambda x: x[1])[0]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, lo probaremos con algunos valores\n",
    "print('(e=5,s=9) =', es_aprobado(5,9))\n",
    "print('(e=0,s=8) =', es_aprobado(0,8))\n",
    "print('(e=3,s=3) =', es_aprobado(3,3))\n",
    "print('(e=5,s=6) =', es_aprobado(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El valor de *k* \n",
    "\n",
    "A pesar de sus ventajas, el desempeño del método *k*-NN es sensible al valor de *k* que es escoja. No hay un valor perfecto para cada aplicación y es necesario probar empíricamente su desempeño con distintos valores. \n",
    "\n",
    "Sin embargo, podemos razonar sobre estos valores. El valor de $k$ estar entre 1 y $N$. Con $k=1$, clasificamos unicamente con respecto al vecino más cercano. Con $k=N$ utilizamos el dataset entero, lo que significa que terminaremos clasificando cualquier valor a la etiqueta más frecuente en el dataset. Véamos cómo se ve esto graficamente con nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un grid de valores para probar\n",
    "XX, YY = np.meshgrid(np.linspace(0, 10, 50), np.linspace(0, 10, 50))\n",
    "pos = np.vstack([XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Probaremos valores de k para 1, 3, 5, 10, 20, 50 y 100\n",
    "k_a_probar = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Probamos y dibujamos\n",
    "for val_k in k_a_probar:\n",
    "    val_x = [ 'green' if es_aprobado(x[0], x[1], k=val_k) == 'APROBADO' else 'red' for x in pos.T ]\n",
    "    plt.scatter(pos[0], pos[1],c=val_x)\n",
    "    plt.xlabel(\"k=\" +  str(val_k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, un valor de $k=1$ produce límites de clasificación que parecen inestables, mientras que uno de $k=N$ clasifica a todos incorrectamente como ``APROBADO``. \n",
    "\n",
    "Hagamos una prueba con validación cruzada para ver la tasa de error para los distintos valores de $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista para acumular las tasas de error\n",
    "errores = []\n",
    "\n",
    "# iteramos con los valores de k\n",
    "for val_k in range(1,100):\n",
    "    suma = 0.0\n",
    "    \n",
    "    # iteramos en todo el dataset\n",
    "    for val_i in range(1,100):\n",
    "        \n",
    "        # tomamos el i-ésimo elemento para probar\n",
    "        td_val = todo_dataset[val_i]\n",
    "        \n",
    "        # utilizamos el dataset menos el i-ésimo elemento como entrenamiento\n",
    "        td_train = todo_dataset[:val_i] + todo_dataset[(val_i + 1):]\n",
    "        \n",
    "        # restamos 1% de error por cada valor correcto\n",
    "        suma += 1 if es_aprobado(td_val[0], td_val[1], k=val_k, datos=td_train) == td_val[2] else 0\n",
    "\n",
    "    errores.append(1.0 - (suma/100))\n",
    "    \n",
    "print(errores)\n",
    "plt.plot(errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, un valor de $k=5$ es óptimo. \n",
    "\n",
    "## Regresión con *k*-NN\n",
    "\n",
    "Es posible hacer regresión con $k$-NN. Para ello, obtenemos el promedio de la variable dependiente (de salida) de los $k$ vecinos más cercanos. \n",
    "\n",
    "Consideremos el siguiente ejemplo. Tenemos un dataset que contiene la ``estatura`` (en cm) y ``peso`` (en kg) de distintas personas y queremos estimar el peso dada la estatura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de peso y estatura\n",
    "estatura = [159, 177, 160, 181, 172, 149, 160, 182, 170, 153, 181, 171, 190, 181, 164, 185, 163, 167, 176, 145, 178, 182, 170, 186, 164, 153, 177, 153, 155, 168, 182, 173, 166, 154, 179, 179, 174, 172, 170, 171, 176, 183, 160, 175, 173, 162, 174, 173, 170, 173, 176, 171, 159, 179, 148, 161, 194, 174, 169, 179, 185, 176, 172, 171, 171, 150, 179, 153, 166, 177, 178, 172, 154, 179, 155, 156, 149, 181, 159, 161, 177, 173, 170, 181, 161, 164, 171, 178, 158, 145, 156, 164, 174, 159, 167, 171, 182, 179, 166, 161, 164, 176, 183, 168, 169, 173, 166, 176, 170, 173, 173, 169, 161, 163, 170, 161, 184, 164, 179, 176, 181, 173, 185, 167, 174, 168, 183, 170, 172, 158, 188, 171, 173, 156, 184, 158, 161, 167, 150, 159, 180, 171, 162, 174, 167, 177, 172, 163, 169, 168, 174, 179, 171, 191, 166, 163, 167, 181, 175, 177, 157, 176, 168, 178, 175, 161, 179, 169, 167, 155, 148, 171, 189, 174, 188, 181, 181, 156, 175, 169, 156, 168, 165, 163, 182, 177, 162, 180, 183, 158, 165, 164, 170, 172, 168, 161, 183, 186, 191, 168]\n",
    "peso = [44, 81, 64, 84, 58, 52, 61, 90, 78, 53, 85, 77, 101, 83, 70, 85, 75, 83, 69, 28, 79, 88, 75, 93, 76, 54, 77, 50, 73, 73, 72, 78, 62, 74, 70, 91, 72, 73, 78, 63, 75, 67, 71, 83, 61, 65, 82, 83, 82, 71, 85, 100, 82, 76, 48, 53, 97, 76, 70, 76, 78, 69, 56, 82, 82, 65, 87, 40, 61, 69, 59, 68, 41, 70, 52, 49, 44, 74, 56, 62, 90, 79, 61, 79, 59, 70, 75, 100, 67, 40, 50, 59, 73, 49, 79, 86, 77, 73, 58, 53, 57, 87, 78, 66, 77, 72, 64, 76, 65, 96, 73, 50, 54, 64, 57, 54, 78, 60, 86, 65, 77, 89, 83, 75, 92, 60, 70, 77, 77, 64, 93, 60, 74, 37, 96, 46, 79, 78, 51, 37, 86, 59, 67, 92, 64, 72, 82, 68, 49, 74, 73, 71, 82, 78, 56, 73, 69, 73, 81, 88, 47, 79, 61, 67, 71, 57, 68, 64, 55, 66, 54, 93, 78, 76, 97, 98, 52, 79, 93, 62, 53, 69, 73, 59, 76, 81, 74, 73, 72, 77, 74, 76, 63, 83, 63, 51, 96, 81, 87, 80]\n",
    "\n",
    "# Definimos nuestra función de regresión usando k-NN\n",
    "def est_peso(mi_estatura, k=20):\n",
    "    dists = [(np.sqrt((mi_estatura - x[0])**2), x[1]) for x in list(zip(estatura, peso))]\n",
    "    dists.sort(key=lambda x: x[0])\n",
    "    \n",
    "    k_vals = np.array([x[1] for x in dists[:k]])\n",
    "    return k_vals.mean()\n",
    "\n",
    "# Datos de prueba\n",
    "vx = np.linspace(145,195,200)\n",
    "\n",
    "# Dibujamos la línea de regresión superpuesta en los datos\n",
    "plt.scatter(estatura,peso,c='lightgray')\n",
    "plt.plot(vx, [ est_peso(x,k=20) for x in vx ], c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras funciones de distancia\n",
    "\n",
    "Existen otras funciones de distancia que nos pueden ser más convenientes de acuerdo a los datos que estemos utilizando. \n",
    "\n",
    "### Distancia euclideana normalizada\n",
    "\n",
    "Un problema con la distancia euclideana es que podemos tener datos con unidades en distintos órdenes de magnitud. Por ejemplo, si tenemos datos de la forma ``(edad, salario)``, donde ``edad`` es un número entre 0 y 120 y ``salario`` es un número entre 0 y 999,999,999 tendríamos el siguiente problema: a la hora de calcular distancias, la variable ``salario`` tendría **mucho más peso** que ``edad`` a la hora de determinar qué tan distantes son estos dos valores.\n",
    "\n",
    "Este problema se puede resolver **normalizando los datos** (con media y desviación estándar) a la hora de pre-procesarlos y **normalizando cada valor de entrada a nuestra función** de clasificación o regresión. O bien utilizando la **distancia euclideana normalizada**, tambien llamada **estandarizada**: $$\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) = \\sqrt{\\sum_{i=1}^{M} \\frac{(a_i - b_i)^2}{\\sigma_i^2}}$$\n",
    "\n",
    "donde $\\sigma_i^2$ es la **desviación estándar** $i$-ésima columna en el dataset.\n",
    "\n",
    "### Distancias Manhattan y Minkowski\n",
    "\n",
    "La distancia Manhattan (también conocida como **norma** $L_1$) es similar a la euclideana, pero en vez de elevar al cuadrado y obtener la raíz de la suma, simplemente utilizamos valores absolutos: $$\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) = {\\sum_{i=1}^{M} {\\mid a_i - b_i \\mid}}$$ La siguiente figura nos ilustra la diferencia entre las distancias euclideana y Manhattan, siendo la primera \"circular\" y la segunda \"cuadrada\" ([Fuente](https://link.springer.com/article/10.1007/s10462-019-09712-9/figures/3))\n",
    "\n",
    "<img src=\"img/Manhattan_Euclideana.png\" width=\"200\">\n",
    "\n",
    "Tanto la distancia euclideana como la Manhattan son **casos especiales** de la denominada **distancia Minkowski**, definida como $$\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) = \\left({\\sum_{i=1}^{M} {\\mid a_i - b_i \\mid}^p}\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "siendo la distancia euclideana la correspondiente a $p=2$ y la Manhattan a $p=1$.\n",
    "\n",
    "### Distancias con datos categóricos \n",
    "\n",
    "Cuando tenemos datos categóricos (p.ej. sexo, provincia), tenemos la opción de utilizar **one-hot encoding** para hacer numéricos nuestros datos y luego utilizar la distancia euclideana. Otra opción es utilizar funciones como\n",
    "\n",
    "$$\\vec{d}\\left(\\mathbf{a},\\mathbf{b}\\right) = \\sum_{i=1}^{M} \\delta\\left(a_i, b_i \\right)$$ donde $$\\delta(x,y) = \\cases{0 \\textrm{ si } x=y, \\\\ 1\\textrm { si } x\\ne y}$$.\n",
    "\n",
    "Existen una gran cantidad de variantes de esta función de distancia, en las que, por ejemplo, el valor de $\\delta(x,y)$ se ajusta a la frecuencia relativa de los valores de $x$ y $y$ en caso de que $x \\ne y$. Una manera de ver esto es imaginarnos un dataset en el que el **95% de los puntos son masculinos**. Intuitivamente, un punto con un valor **femenino debería tener un valor de distancia con bastante más peso**, por la diferencia de frecuencia que esto conlleva. \n",
    "\n",
    "\n",
    "## Desventajas de *k*-NN\n",
    "\n",
    "*k*-NN es un método muy versátil y poderoso, pero tiene varias falencias que debemos detallar:\n",
    "\n",
    "- Al ser un método en el que no se estiman parámetros en una fase de entrenamiento, tenemos que \"cargar\" con el dataset. Esto dificulta su uso en aplicaciones con datasets muy grandes.\n",
    "\n",
    "- El método es vulnerable a la **maldición de la dimensionalidad** que estudiaremos pronto.\n",
    "\n",
    "\n",
    "## *k*-NN con ``scikit-learn``\n",
    "\n",
    "La clase de ``scikit-learn`` para hace clasificación *k*-NN es ``sklearn.neighbors.KNeighborsClassifier`` y sigue la misma convención de esta biblioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la biblioteca\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Creamos el objeto knn donde definimos el valor de k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Hacemos el fit contra los datos\n",
    "knn.fit(np.array((horas_estudio, horas_sueno)).T, resultado_examen)\n",
    "\n",
    "# Lo evaluamos contra el grid de posiciones generadas anteriormente\n",
    "preds = knn.predict(pos.T)\n",
    "\n",
    "# Y lo dibujamos \n",
    "val_zzz = ['green' if x == 'APROBADO' else 'red' for x in preds ]\n",
    "plt.scatter(pos[0], pos[1],c=val_zzz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer regresión, la clase correspondiente es ``sklearn.neighbors.KNeighborsRegressor``.\n",
    "\n",
    "Véamosla en acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la biblioteca\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Creamos el objeto kNN\n",
    "knn = KNeighborsRegressor(n_neighbors=20, weights='uniform')\n",
    "\n",
    "# Hacemos el fit de estatura contra peso\n",
    "knn.fit(np.array([estatura]).T, peso)\n",
    "\n",
    "# Hacemos las predicciones contra nuestros datos de ejemplo\n",
    "vals = knn.predict(np.array([vx]).T)\n",
    "\n",
    "# Dibujamos la línea de regresión\n",
    "plt.scatter(estatura,peso,c='lightgray')\n",
    "plt.plot(vx, vals, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*-NN con Cross Validation (cross_val_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la biblioteca\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Creamos el objeto knn donde definimos el valor de k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# sacamos el cross val score\n",
    "print(cross_val_score(knn, np.array((horas_estudio, horas_sueno)).T, resultado_examen, cv=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
